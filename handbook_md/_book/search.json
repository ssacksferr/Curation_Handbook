[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UNHCR Microdata Curation Handbook - 2023",
    "section": "",
    "text": "About the handbook\nThis handbook is a technical guide for curating personal microdata of persons of concern (PoCs) to UNHCR, and uploading this data internally on the Raw Internal Data Library (RIDL) and publishing it externally on UNHCR’s Microdata Library (MDL). It is complementary to the Administrative Instruction, Curation of Personal Microdata of PoCs to UNHCR, herein referred to as the ‘Curation AI’, and was written with the assumption that any user of the handbook has read and understands the Curation AI.\nFor guidance on curation and anonymization of specific UNHCR surveys, visit this page instead. Access can be granted by the Curation Team."
  },
  {
    "objectID": "intro.html#sec-2purpose",
    "href": "intro.html#sec-2purpose",
    "title": "1  Introduction",
    "section": "1.1 Purpose and objective",
    "text": "1.1 Purpose and objective\nThe purpose of this handbook is to provide detailed technical guidance for Data Curators on the curation of personal microdata of PoCs. The objective is to ensure that the implementation of the Data Curator role is in line with the Curation AI and done in a consistent manner and following best practices.\nNote that any mention of microdata or microdataset or, more generally, data or dataset refers to personal microdata of PoCs unless otherwise stated."
  },
  {
    "objectID": "intro.html#sec-2audience",
    "href": "intro.html#sec-2audience",
    "title": "1  Introduction",
    "section": "1.2 Audience",
    "text": "1.2 Audience\nThe primary audience is UNHCR personnel performing the Data Curator role in the curation of microdata of PoCs. The handbook was written with the assumption that anyone using it already has a basic to advanced knowledge of data treatment, statistics, anonymization and statistical applications such as R and Stata.\nSecondary audiences of this handbook include any UNHCR personnel involved in data collection, processing and dissemination and any UNHCR personnel involved in the data curation process who are interested in or need to know specific details on how microdata is curated. This includes Personal Data Controllers, Data Protection Focal Points and Data Providers."
  },
  {
    "objectID": "intro.html#sec-2scope",
    "href": "intro.html#sec-2scope",
    "title": "1  Introduction",
    "section": "1.3 Scope",
    "text": "1.3 Scope\nThis handbook includes technical guidance on the following aspects of data curation within the scope of what is performed by UNHCR under the Curation AI: data checking and preparation, anonymization, disclosure risk assessments and metadata development.\nThis handbook does not include guidance on data collection, processing and analysis for primary use, even though some of the techniques used may also be transferable to those stages, particularly related to data checking and preparation. Finally, it does not include technical guidance on the use of RIDL or the MDL as this guidance is already available from the developers. They can be found below:\n• RIDL: https://im.unhcr.org/ridl/\n• MDL: will be added soon because the platform is undergoing an upgrade\nIn the context of UNHCR’s personal microdata of PoCs, the microdata curation process involves seven steps as follows:\n\n\n\n\n\nThis handbook focuses on the technical aspects of steps three to seven, starting after raw or clean data have been downloaded from RIDL to be checked, prepared and anonymized for publication on the MDL.\nDifferent versions of a microdataset are used and/or produced throughout the curation process. They are generally categorized as raw, clean and anonymous. The following table from the Curation AI paragraph 4.3.3 provides a definition of each version and an overview of their purpose.\n\n\n\n\n\n\n\n\n\nVersion\nDefinition\nPurpose\nLocation\n\n\n\n\nRaw\nPersonal or non-personal data in its original format as produced without any edits.\nInternal reference and use\nRIDL and other systems and tools like proGres v4, KoBo, etc.\n\n\nClean\nPersonal or non-personal data that has been processed, i.e., structured, treated for errors and verified1\nInternal re-use by UNHCR at the country, regional and/or global levels\nRIDL and other systems and tools like proGres v4, KoBo, etc.\n\n\nAnonymous\nClean personal data that has undergone a technical process of removing or modifying all personal identifiers and codes in such a way that individual data subjects cannot be identified by any means reasonably likely. This process is complemented by other measures to render the risk of re-identification insignificant2\nInternal re-use by UNHCR at the country, regional and/or global levels\nRIDL and the MDL\n\n\n\n\nRe-use by external actors for programming, analysis, research, advocacy, etc."
  },
  {
    "objectID": "intro.html#sec-2contact",
    "href": "intro.html#sec-2contact",
    "title": "1  Introduction",
    "section": "1.4 Contact",
    "text": "1.4 Contact\nIf you have questions that are not answered in this handbook, please contact microdata@unhcr.org."
  },
  {
    "objectID": "getting_started.html#sec-3filemgmt",
    "href": "getting_started.html#sec-3filemgmt",
    "title": "2  Getting started",
    "section": "2.1 File management and naming conventions",
    "text": "2.1 File management and naming conventions\nTo maintain order, structure, consistency and transparency across the organization, standards for naming conventions and file management are important. The following are the standards must be used by Data Curators.\n\n2.1.1 Folder structure\nEach dataset that will be curated and published on the MDL must be downloaded from RIDL, checked, prepared, and anonymized on your local computer (i.e., using R as highlighted throughout this Handbook). Not all files associated with the curation will be stored on RIDL including the curation script, the disclosure risk assessment report, the MDL metadata population script and any files that will no longer be needed after the curation is finished. To maintain transparency within the organization, all files associated with this process are stored in a secure repository on the GDS – Data Curation private SharePoint space. The metadata of external datasets that are harvested for the MDL should also be stored in this folder.\nFiles are organized by region and country where the data was collected. Beyond this, each dataset has a dedicated main folder. The name of this folder is the same as the ‘Survey identifier’ (see next section). Within this folder, are a set of subfolders organized in a way so that anyone can easily retrace the steps of the curation process. They should be organized into folders using the logic below.\n\n\n\n\n\n\n\nFolder\nContent\n\n\n\n\n0_data\nAll versions of the data. There should be at minimum three subfolders associated with the different versions: raw, clean and anonymous\n\n\n1_scripts\nAll scripts used in the curation of the dataset including the script(s) use for data check and preparation, anonymization and disclosure risk assessment script.\n\n\n2_documentation\nThe metadata script for the dataset as well as the supporting documents including the questionnaire, report and any other analytical pieces produced (e.g briefs, infographics, etc.).\n\n\n3_authorization\nThe final version of the disclosure risk assessment report and copy of the email from the Personal Data Controller providing or rejecting validation of the anonymization and authorization for release on the MDL1.\n\n\n\nNote that Data Curators are encouraged to use R projects, which facilitate the use of relative paths so that R scripts are easily used by multiple users on different machines. The R project should be stored in the main directory of the dataset establishing it as the root folder. See more details on this in section Software requirements and setup .\n\n\n2.1.2 Dataset identifier\nThe dataset identifier (also referred to as a survey identifier by certain DDI compliant catalogues) is a unique code that can be used to identify any file associated with the dataset. For example, it will be the name of the folder on SharePoint where all working files related to the curation are stored. It is also the base for file names (see next section). The dataset identifier must:\n\nnot include any spaces or special characters except for underscores (_);\nonly include standard English letters in upper case and numbers;\nseparate the different components of the identifier (see below) with underscores2 (_); and\nnot share the same identifiers as another dataset.\n\nThe dataset identifier is made up of the following components in the order it is presented below: \n\nSource: The source of the dataset (e.g. UNHCR)\nCountry code: ISO3 code of the country covered in the field (e.g. AFG for Afghanistan). See list of ISO3 codes here >> \nYear: year to which data collection started, in the format yyyy (e.g. 2019) \nInformation type: short description of the information type (e.g. PDM, SENS, etc.).\nOther specification (optional): if necessary, another field can be added to distinguish different datasets that would have otherwise the same identifier (e.g. a dataset comprised of multiple files, the same survey may be carried out in the same country and year but in different camps or times).\n\nThe following is an example identifier for a standardized expanded nutrition survey (SENS) undertaken by UNHCR in Zimbabwe in 2017 in the camp number 1: UNHCR_ZWE_2017_SENS_CAMP1.\nAn example of file types and their suffixes is shown in the following table:\n\n\n\nFile type\nSuffix\n\n\n\n\nraw data\n_data_raw_v0.1\n\n\nclean data\n_data_clean_v1.1\n\n\nanonymous data\n_data_anon_v2.1\n\n\nquestionnaire\n_questionnaire\n\n\nreport\n_report\n\n\ninfographic\n_infographic\n\n\nmap\n_map\n\n\ncleaning script\n_script_clean\n\n\nanonymization script\n_script_anon\n\n\ndisclosure risk report\n_dra_report\n\n\n\n\n\n2.1.3 Data file versions\nAs mentioned under scope, there are three version of a microdataset that are used and/or produced in the curation process. The file version in the RIDL and MDL metadata editor and attached to the file name should be as follows:\n\n\n\nFile version\nExtension\n\n\n\n\nraw\nv0.x\n\n\nclean\nv1.x\n\n\nanonymous3\nv2.x\n\n\n\nSo, for example, the first version of the clean data for a socio-economic assessment undertaken by UNHCR in Zimbabwe in 2017 in the camp number 1 would be UNHCR_ZWE_2017_SEA_CAMP1_data_clean_v1.1.csv and the anonymized version would be UNHCR_ZWE_2017_SEA_CAMP1_data_anon_v2.1.csv.\n\n\n2.1.4 Dataset title\nThe dataset title should follow the naming convention used by UNHCR’s Operational Data Portal. Survey names in RIDL are preferably in English, however it is acceptable if they are in the language of the report and dataset. In the Microdata Library, the survey name should be in English. Guidance below.\n\nLocation: The title of the dataset should always begin with the location at either country or regional level. The location name (followed by a colon) should be added at the beginning of the title. For example: Uganda: Rhino Settlement Profile - April 2019, instead of Settlement Profile - Rhino - April 2019. In case the dataset refers to multiple countries or a situation, the name of the region or situation could be added, e.g. Burundi Situation: Population Dashboard - 30 April 2019.\nNote that the country name should not be included when the dataset is uploaded to the MDL because it is automatically fetched from the metadata and would cause a repetition. In the case however that the dataset on MDL refers to a specific location within a country, the specific location should be included in place of the country name.\nTitle: The title of the dataset follows the location after a colon and should be descriptive of the type of content (survey, assessment, profiling exercise, etc.). As part of DDI-compliant metadata, and if the dataset has a title in a language different than English, an alternate title can be added in the original language.\nDate: The year should be added at the end of the title. The full date should be added when two (or more) surveys, often belonging to a series, have the exact same title. The date, in a [Day] Month Year format should be added at the end of title, e.g. Lebanon: National Inter-Sector List - May 2019.\nCapitalization: Titles and descriptions should be written in title case. This means only using capital letters for the principal words, e.g. Zambia: Sector Updates - July 2019.\n\nExamples:\nZimbabwe: Socio-economic Assessment in Camp 1 – 2017\nSouth Sudan Situation: Population Dashboard - April 2019"
  },
  {
    "objectID": "getting_started.html#sec-3setup",
    "href": "getting_started.html#sec-3setup",
    "title": "2  Getting started",
    "section": "2.2 Software requirements and setup",
    "text": "2.2 Software requirements and setup\n\n2.2.1 Software requirements\nR and R Studio are the software packages used in the examples in this handbook. GDS SDS prefers Data Curators to use R to ensure the curation process is adequately documented, transparent, replicable and to avoid any compatibility issues. For example, data manipulation in MS Excel cannot be tracked because it does not include a script and Stata is a proprietary software not accessible to everyone.\n\n\n2.2.2 Installing R and R Studio\nInstall R from https://cloud.r-project.org/ and R Studio from https://www.rstudio.com/products/rstudio/download/#download. \nFrom time to time, you will need to update R. When using Windows, this can be done directly in RStudio using the installr package.\n\n\nCode\n#Check R version\nsessionInfo() # your current version will display in the first line of the result\n\n# Install and load installr package\ninstall.packages(\"installr\")\nlibrary(installr)\n\n# Update R version\nupdateR()\n\n\n\n\n2.2.3 Setting up a R project and creating an R script\nThe first step is to set up an R project. Using R projects facilitates your work to be easily reproduced by Data Providers and other Data Curators because it facilitates relative paths. See an article about it here.\nTo set up a project:\n\nFirstly, make sure you have created a folder for your dataset that follows the structure and naming conventions outlined in the previous section.\nOpen R Studio and create a new project (File > New Project)\nSelect “Create Project from Existing Directory” and select the main directory of the dataset you created (Note: this will become the root directory from which all your paths will start). See example below:\n\n\n\n\n\n\nNow anytime you work on that project, start by opening the project and then create your scripts (New File > R Script) or navigate to them from there.\n\n\n2.2.4 Installing packages\nOnce you have installed RStudio, set up your project and started a new script you will also need to install any packages that you will use in your script. For anonymization, you will need the sdcMicro package. You should work directly in the RStudio interface, however the sdcMicro GUI application, which does not require writing R code, may be useful for testing or to those less familiar with R. That said, due to the limited functionality of the GUI application, Data Curators must eventually become familiar with R and interact with the sdcMicro package using the R interface.\nInstalling sdcMicro in R\n\n\nCode\n# Install package\ninstall.packages(\"sdcMicro\")\n\n# Load package\nlibrary(sdcMicro)\n\n# Load sdcMicro GUI\nsdcApp()\n\n\nSeveral other packages are used in the examples in this handbook for the curation process. They are listed in Annex – R packages. The following provides example code to install and load multiple packages at the same time.\nInstalling multiple packages in R\n\n\nCode\n# Check what packages are installed on your computer\ninstalled.packages()\n\n# List of packages to install and load. Add more to list if needed. This manual assumes these packages are installed. \npackages_needed <- c('your_package_name','another_package_name',\n                     'and_another_package_name')\n\n# Run function to install and update packages. If one of the packages shows up as FALSE, try to install again.\nrepos <- c(CRAN = 'https://cran.rstudio.com', CRANextra = 'https://macos.rbind.io')\n \ninstall.packages(packages_needed[!packages_needed %in% row.names(installed.packages())],\n                 dependencies = TRUE,\n                 repos = repos)\n \nupdate.packages(oldPkgs = packages_needed, ask = FALSE)\n \nsapply(packages_needed, require, character.only=TRUE)\n\n\nNote There are many ways to complete a task in R, and a variety of packages that can be used to reach the same objective. The example code provided throughout this handbook may not be / is probably not the only way. For users who prefer different methods, use them. The most important point is that the task is completed, and the result is correct. Suggestions for more efficient methods are most welcome to microdata@unhcr.org.\n\n\n2.2.5 Importing data\nMicrodata on RIDL come in a variety of formats but are usually in at least .csv or .xls(x), and sometimes stata (.dta) or spss (.sav).\n\n\nCode\ndat_raw <- read_csv('0_data/raw/file_name.csv')\n\n# Import .xls or .xlsx using the readxl package\ndat_raw <- read_excel('0_data/raw/file_name.xls',sheet='tab_name', na=c('', 'NA', 'missing')\ndat_raw <- read_excel('0_data/raw/file_name.xlsx',sheet='tab_name')\n\n# Import .dta and .sav files using the haven package\ndat_raw <- read_dta('0_data/raw/file_name.dta')\ndat_raw <- read_sav('0_data/raw/file_name.sav')\n\n\nSometimes issues arise when importing the data into R. Below are some common issues and fixes, obviously not an exhaustive list.\nPotential issues and solutions when importing data into R\n\nA csv file contains accents or special characters such as é, ô, etc.:  Specify the parameter encoding = ‘UTF-8’ in read_csv\nAn Excel file contains records with line breaks: R will misalign the data, creating new records that are part of another record. Using the readxl package should resolve this issue, however not always as it depends on the encoding of the source file. The line breaks can be removed directly in Excel using ‘find and replace’ (see below).\n\n\n\n\n\n\n\nVariable/column types are read incorrectly by R: By default, R uses the first 1000 observations to ‘guess’ the variable type. If it is an xls(x) or csv file , then guess_max option can be used to increase the number of rows used to guess the variable type."
  },
  {
    "objectID": "indentify_data.html#sec-4datacheck",
    "href": "indentify_data.html#sec-4datacheck",
    "title": "3  Identify data for publication",
    "section": "3.1 Data collected using proGres v4",
    "text": "3.1 Data collected using proGres v4\nFor personal microdata of PoCs collected and managed using proGres v4, the microdata is uploaded to RIDL on an annual basis. The protection team in each operation should meet with the personnel identified to perform the Data Curator role for a given dataset on an annual basis, and address the following:\n\nIdentify if there is any reason not to publish an anonymous version of the data on the MDL (see section 5.3 of the Curation AI).\nIdentify if there is any sensitive data or information in the proGres v4 on RIDL.\nDetermine the variables and level of detail that need to be preserved in the microdataset.\n\nNote that the initial meeting with a given country operation before the first release of personal microdata of PoCs from proGres v4 will potentially require more time to go over the context, curation objectives and planning, while meetings in the subsequent years may simply aim to determine if the operational context and/or protection risks have changed in a manner that would impact the methods used in microdata curation and the utility of the data."
  },
  {
    "objectID": "indentify_data.html#sec-4outsideprogres",
    "href": "indentify_data.html#sec-4outsideprogres",
    "title": "3  Identify data for publication",
    "section": "3.2 Data collected outside of proGres v4",
    "text": "3.2 Data collected outside of proGres v4\nFor personal microdata of PoCs collected outside proGres v4, the Data Curator may identify data for curation following several different scenarios, such as:\n\nAfter consulting with the Personal Data Controller and/or Data Protection Focal Point, the Data Provider notifies the Data Curator each time they upload personal microdata of PoCs to RIDL and believe it may be suitable for external publication on the MDL.\nUNHCR personnel who regularly perform the Data Curator role, DIMA units and country-level IMOs maintain regular contact and monitor RIDL to identify personal microdata of PoCs that may be suitable for external publication. They may approach the Data Providers at any time.\nA third party may specifically request access to microdata held by UNHCR, and a Data Curator is notified of this request so they can provide support. \n\nA registry of personal microdata of PoCs developed and/or maintained by the Personal Data Controller and Data Providers (see Curation AI paragraph 5.2.3) can serve as a useful tool in identifying microdata collected outside of proGres v4 for curation. The inventory should include some of the following useful details:\n\nThe title of the personal microdata of PoCs collected;\nLocation (region and country);\nYear\nIf the data were catalogued on RIDL (yes/no);\nlink to the dataset on RIDL (if applicable);\nIf the data are relevant for anonymization and publication on the MDL (yes/no);\nIf they data were curated and published on the MDL (yes/no); and\nPersonal Data Controller, Data Provider and Data Curator names.\n\nBelow is a very simplified example of a registry. Note that this could be integrated into an existing registry developed by the country operation or regional bureau, simply adding the fields that are helpful to track the storage of microdata on RIDL, anonymization and publication on the MDL.\n\nExample of registry\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nYear\nSource\nDataset\nRIDL\nMDL\nData provider\nData Curator\n\n\n\n\nCMR\n2016\nUNHCR\nMinawao: SENS\nyes\nyes\nJohn Doe\nJane Doe\n\n\n\nDatasets that change frequently\nFor datasets that have frequent updates, such as monitoring exercises repeated on a regular interval (monthly, quarterly, yearly, etc.), panel surveys, etc., each update is treated as a separate (new) dataset. The metadata should specify the coverage of the dataset, allowing Data Users to understand the difference between each version of the dataset."
  },
  {
    "objectID": "indentify_data.html#sec-4compileinfo",
    "href": "indentify_data.html#sec-4compileinfo",
    "title": "3  Identify data for publication",
    "section": "3.3 Compile information required for the disclosure risk assessment",
    "text": "3.3 Compile information required for the disclosure risk assessment\nOnce microdata is identified, the Data Curator needs to confirm that all the information required for the disclosure risk assessment (DRA) is available. Most of this information should be in the report (if available) and/or the RIDL metadata, however there are some details that are crucial that may not be available.\nThe Data Curator can compare what information is available to what is required in the DRA checklist and reach out to the Data Provider to compile any missing or incomplete information as needed. The following is the DRA checklist from Annex D of the Curation AI.\n\nWhat are the potential disclosure risk scenarios – e.g., realistic assumptions about who may be interested in the microdata and for what purpose – and available data and information (both internal and external) covering the same population group that could be linked to this personal microdata (e.g., through the mosaic effect)?\nIs there any sensitive data or information in the dataset, including sensitive personal data? If yes, what are they? Some commonly found in UNHCR’s microdata include, for example:\n\n\ngeographic location (current and/or former location if displaced population);\nage, gender and diversity considerations such as language, ethnicity, religion;\noccupation, livelihood activity, income level, etc.; and\nvariables that are related or can be linked to the sensitive variables.\n\n\nWhat are the ‘key variables’ or variables that can be used to indirectly identify a data subject or be linked to another variable that can then be used to identify a data subject? Some commonly found in UNHCR’s microdata include, for example:\n\n\ngeographic location (current and/or former location if displaced population);\nhousehold size, head of household marital status, specific vulnerabilities within the household;\nage, gender and diversity considerations such as language, ethnicity, religion; and\noccupation, livelihood activity, income level, etc.\n\n\nIf the microdata is a sample, what is the sample design and weights? Note that if the sample design included stratification, the weights need to be calculated for each stratum.\nWhich cleaning measures were already taken, including the treatment of outliers, grouping/re-grouping of variables, etc.?\nWas the personal microdata of PoCs collected directly by UNHCR, by a partner on behalf of UNHCR, jointly with a partner, or jointly with another third party?\nAre additional technical, organizational or legal measures or otherwise binding commitments necessary to be put in place to reduce the risk of disclosure? These measures and commitments are context specific. Some commonly found examples include:\n\n\naccess control and management\nstatistical disclosure control (SDC)\nencryption\nbilateral or multilateral data sharing or collaboration agreements\nTerms of Use\nother contractual or written commitments\ntraining for personnel\netc."
  },
  {
    "objectID": "indentify_data.html#sec-4externaldata",
    "href": "indentify_data.html#sec-4externaldata",
    "title": "3  Identify data for publication",
    "section": "3.4 Data from external providers",
    "text": "3.4 Data from external providers\nBy default, and when requested by a data provider, the MDL will host metadata on data made available externally by other institutions. Each study will hold a DDI file, but no additional documentation will be hosted in the MDL unless agreed upon by the data owning institution. Data will be hosted on the MDL if it is agreed with the data controlling institution.\nRequests to upload metadata from external providers that are hosted externally are evaluated case-by-case, based on the rationale provided in section 3.2 of Administrative Instruction on the Curation of Personal Microdata of Persons of Concern to UNHCR. If data is requested to be made available, the data provider will be requested to meet the conditions outlined in section 5.3.3 of that Administrative Instruction.\nWhen data is not hosted in the MDL, UNHCR’s MDL will not handle data requests for external datasets. For institutions that require UNHCR’s MDL to do so, the data requests will be handled under the same procedures as UNHCR’s datasets.\n\n3.4.1 Procedures for specific platforms\n\n3.4.1.1 OCHA HDX\nFrom UNHCR – MDL to HDX\n\nHDX has set up an automatic scraper that extracts data from the UNHCR NADA server (external facing platform)\nCustodian of scraper is HDX\nScraper creates one entry per each dataset to be stored under a UNHCR collection\nScraper only collects data that is identified in the field ‘ID’ as beginning with prefix “UNHCR_”, as to avoid double harvesting\nIn order to maintain versions unified, access to data is handled by redirecting users back to UNHCR request page to download the data.\n\nFrom HDX to UNHCR – MDL\n\nUNHCR has written an R script that searches for keywords of HDX datasets using the R client of HDX\nCustodian of script is HCR\nWhen datasets hit the queries, UNHCR complements the existing metadata available in HDX to make it DDI compliant\nDatasets with created documentation are to be stored under a HDX collection\nIn order to maintain versions unified, access to data is handled by redirecting users back to HDX request page to download the data.\nHDX datasets are not stored in UNHCR’s internal platform\n\n\n\n3.4.1.2 WB Microdata Library\nFrom UNHCR – MDL to WB MDL\n\nWB uses NADAR package to extract metadata at the study and variable level from UNHCR NADA server (external facing platform)\nWB sometimes amends metadata slightly to adhere to internal metadata standards\nDatasets created by UNHCR, i.e. those with the prefix “UNHCR_” in their ID, are to be stored under a UNHCR collection\nIn order to maintain versions unified, access to data is handled by redirecting users back to UNHCR request page to download the data.\nDocumentation like reports, questionnaires and metadata files are available in both platforms\n\nFrom WB MDL to UNHCR - MDL\n\nUNHCR uses keywords to identify suitable datasets for its MDL on WB NADA server\nUNHCR uses MDL package to extract metadata at the study and variable level from WB NADA server (external facing platform)\nIn order to maintain versions unified, access to data is handled by redirecting users back to WB request page to download the data.\nDocumentation like reports, questionnaires and metadata files are available in both platforms"
  },
  {
    "objectID": "anonymize.html#sec-5datacheck",
    "href": "anonymize.html#sec-5datacheck",
    "title": "4  Anonymize",
    "section": "4.1 Data check and preparation",
    "text": "4.1 Data check and preparation\nThe objective of the data check and preparation is to identify any issues in a dataset (errors and inconsistencies across variables, data incompleteness, etc.) and ensure that:\n\nthe statistics shared in public reports are reproducible; and\nthe dataset is ready for re-use by other software packages and users (i.e. variable names and labels are logical, data is in a standard and reusable format, etc.)\n\nA few points to note:\n\nIdeally any issues in a dataset should have been addressed by the Data Provider as part of the primary data processing and use, and the data shared on RIDL is a clean version. Nonetheless, often the only data available on RIDL is the raw version and/or the clean version may still have some issues that need to be addressed before the data is anonymized and published on the MDL.\nThis step may reveal quality issues that would require a reconsideration of the relevance of the publication of the anonymous version of the dataset and/or put into question the results of an already published report or analytical piece. If this is the case, the Data Curator must present their concerns to the Data Provider.\nData Curators should take a conservative approach and any issue should be fixed only if necessary and only after discussing it with all the Data Providers and receiving their approval.\nIf that dataset requires any cleaning, it must be documented, preferably in a R script or other software package, to ensure the replicability of the results. A copy of the raw data (e.g. 0.1) or previous versions of the clean data prior to further cleaning (e.g. 1.1) shared on RIDL should always be preserved.\nThis section focuses on the most common issues found in personal microdata of PoCs collected by UNHCR and its partners, however every dataset has its own particularities and may require additional checks.\n\nData check and preparation checklist\n\nCheck variable names and labels\nCheck value name and labels\nCheck variable order\nCheck variable types\nCheck data completeness\nCheck for duplicate records\nCheck for missing values\nCheck for excess spacing in string values\nCheck for consistent responses\nCheck for consistent variable relationships\nCheck for outliers\nCheck consistency with primary analysis / report\nCheck consent variable\nCheck and prepare unique ID\nSave clean version and cleaning script\nRemove variables that will not be released\nPrepare sample (if relevant)\nPrepare weights\n\n \n\n4.1.1 Variable names and labels\nVariable names (or codes) are the short names used by a software to refer to or call a variable, whereas variable labels are a description of the variable, reflecting the question used in a questionnaire or instructions in a data collection form. At the very least, variable names and labels must be easily interpreted and follow the same style throughout the data. If standards or codebooks are not available from the Data Provider or within UNHCR and/or are not consistent or easily interpreted, the Humanitarian Exchange Language (HXL) should serve as a guide together with the general instructions below.)\nThe following table highlights how names and variable labels are referred to in R, Stata and Kobo, and any limitations.\n\n\n\n\n\n\n\n\n\n\nSoftware\nVariable name\n\nVariable label\n\n\n\n\n\n\nHow it is called in the software\nLimitations\nHow it is called in the software\nlimitations\n\n\nR\nname\nNo spaces\nlabel\nNone\n\n\n\n\nWill run into issues if use special characters such as @ or start name with an underscore (_) or number\n\n\n\n\n\n\nCannot use words reserved for functions\n\n\n\n\nStata\nname\nNo spaces\nlabel\n80-character limit\n\n\n\n\n32 characters\n\n\n\n\n\n\nFirst character must be a letter or one of the characters @, #, or $\n\n\n\n\n\n\nCannot use words reserved for functions\n\n\n\n\nKoBo\nData Column Name in form creator\nNo spaces\nQuestion in form creator\nNo restrictions\n\n\n\nXML values in data download\nNo limit on # of characters\nLabel in data download\n\n\n\n\n\nOnly letters, numbers, and underscores are allowed\n\n\n\n\n\n\nMust start with a letter or an underscore\n\n\n\n\n\n \n\n4.1.1.1 Variable names\nVariable names should be concise, easy-to-read and, where possible, follow standards established or followed by UNHCR and its partners. In general, the following rules should be applied:\n\nVariable names should be no longer than 32 characters.\nVariable names should only include standard English letters (a,b,c…z), numbers (0,1,…9) and underscore (_). \nVariable names should not include special alphabet letters (æ, ñ, é, etc.), forward/back slashes (/,), periods (.) or any other special characters ($, ’’, *, @, etc.) or spaces.\nVariable names should always start with a letter. Do not start a name with a number or an underscore.\nVariable names should be in lowercase when possible\n\nWhile there is no style to follow strictly, once you choose a style you should stick with it for all the variables in the dataset. Common styles are underscore (_) separate words (for example: your_variable_name) or camel case (for example: yourVariableName). Underscore separated lowercase words is the preferred style.\nIf variable names refer to the questions of a survey, they should reflect the structure of the questionnaire. For example, question 4 of section 7 could be named as S07_04. If there are several alternatives in said question and each one is represented in a different column of the dataset, they could be named ordinally as S07_04_a.\nIf the same variable appears in multiple data files, it should have the same name (for example, the household ID may be present in both the household level file and the individual level file). On the other hand, if a variable is named equally across more than one data file but it refers to different information, one of them should be renamed\nIf the same variable name appears in the same data file more than once, one of the variables should be renamed.  \nRenaming variables in R\n\n\nCode\n#Given the dataset your_dataset – we will use this name in all examples -, you can print the variable names with the following command:\n# print on console all variable names \nnames(your_dataset)\n\n#You can rename all variable names with the following command (please note that you must provide a name for all the variables): \n# Example of converting the names in a data frame with 4 variables \nnames(your_dataset) <- c(\"new_var_name1\", \"new_var_name2\", \"new_var_name3\", \"new_var_name4\") \n\n#If you just need to modify the name of one or few variables, the following code may be used:\n# Convert the name of one variable\nnames(your_dataset) [names(your_dataset) == \"old_var_name\"] <- \"new_var_name\"\n\n#And alternatively, you can use the dplyr package: \n# Rename specific columns \nyour_dataset <- rename(your_dataset, new_var_name1 = old_var_name1,  new_var_name3 = old_var_name3) \n\n#Several other issues in the data can be easily and efficiently managed in R such as converting all variables to lowercase, replacing characters in variable names and trimming variable length. Example codes below:\n# Convert all variable names to lowercase\nnames(your_dataset) <- tolower(names(your_dataset))\n\n# Change forward slashes in variable names to underscores\nnames(your_dataset) <- gsub(x = names(your_dataset), pattern = \"/\", replacement = \"_\")\n\n# Limit number of characters to <32. Pay attention b/c it may make two or more variables have the same variable name!!!\nnames(your_dataset) <- strtrim(names(your_dataset), 31)\n\n#The clean_names function in the janitor package is also a useful tool to clean names, with a number of options. Read more about it here: https://www.rdocumentation.org/packages/janitor/versions/1.2.0/topics/clean_names\n\n\nExporting variable names from Kobo\nVariables in Kobo have labels (i.e., the question asked in the form) and XML values (i.e., name of the variable). While by default data downloaded from Kobo uses the complete questionnaire questions as column headers, data uploaded to RIDL should have the XML values from Kobo as variable name. To do this:\n1. Set the “Value and header format’ to ‘XML values and headers’ when downloading data from Kobo. See screenshot below:\n\n\n\n\n\n2. Ensure the RIDL record contains the Kobo form, which serves as both the questionnaire and data dictionary with the labels and names (XML values).\n \n\n\n\n4.1.2 Variable order\nThe order in which variables (columns) appear in a data should be sensible and meaningful. Typically, unique IDs should come first, and other variables should appear in an order that makes it simple to understand the structure of the dataset. If the data was gathered through a survey, the variables order should reflect the questionnaire structure. \nChanging variable order in R\n\n\nCode\n# using dplyr: specify the variable to move \nvariable_name <- \"your_variable_name\"\n\n# Move as first variable of the dataset \nyour_dataset <- select(your_dataset, variable_name, everything())\n\n# Move as last variable of the dataset \nyour_dataset <- select(your_dataset, - variable_name, everything()) \n \n# Move the variable to the third position\nnew_variable_position <- 3 \nyour_dataset <- select(your_dataset, 1 : new_variable_position, variable_name, everything()) \n\n# Move var_y after var_x using tidyr\nyour_dataset <- your_dataset %>% relocate(\"var_y\", .after = \"var_x\")\n# Move var_y before var_x\nyour_dataset <- your_dataset %>% relocate(\"var_y\", .before = \"var_x\")\n\n#Alternatively, you can specify the exact order of all the variables with the following code. In this case you must list out all the variables in the dataset.\n# Write ALL the variables\" names in the desired order \n# in this simple example, the dataset has only 3 variables \nnew_variable_order <- c(\"var_x\", \"var_y\", \"var_z\") \n \n# Update the dataset with the new order \nyour_dataset <- your_dataset[ , new_variable_order] \n\n\n \n\n\n4.1.3 Variable types\n\n4.1.3.1 Variable type\nDepending on the software application used to collect and store the data, different types of data may be treated as different variable types. For example, when exporting data to an Excel file from Kobo Toolbox, numbers are often treated as text and as such will be considered characters when brought into R. Depending on the usage of the variable, it may need to be converted before performing a given operation. For example, you cannot use the ‘summary’ command on a variable of type character, it needs to first be converted to numeric.\nThe following is a list of the most common variable types or classes in R1:\n\nNumeric – numbers and decimals\nInteger – numeric data without decimals\nCharacter – text or string\nFactor –Integer values/levels with an associated string which is read as a label\nLogical/boolean – Variable with values TRUE/FALSE\n\nNote If you import a dataset into R, and all the variables are treated as characters (for example), it is not necessary to convert every single variable into their correct format in preparing the clean or anonymized version of the data. Focus on the variables that need to be in the correct format for any manipulation you need to do. For example, if you need to examine a numeric variable, it should be of numeric type. If a variable will be a key variable in sdcMicro, it needs to be of type factor (more on that in the next chapter).\nChecking and converting variable formats in R\n\n\nCode\n# Check the variable format\ntypeof(your_dataset$your_variable)\n\n#You can convert the format using the as.character, as.factor, as.numeric and as.integer commands. Below is an example to convert to numeric. The code is the same for the other commands.\n\n# Convert a single variable to numeric\nyour_dataset$your_variable <- as.numeric(your_dataset$your_variable)\n\n# Convert a range of variables from your_variable_r to your_variable_y to numeric\ndat_clean <- mutate(dat_clean, across(your_variable_r:your_variable_y, as.numeric))\n\n\n \n\n\n4.1.3.2 Multiple choice variables\nSome data collection tools, including KoBo Toolbox, include both the short and wide form of a multiple-choice question with multiple select options. For example, the dataset will have one variable with a list of all the responses that the respondent selected, and a binary variable associated with each response option (1 if the response was selected and 0 if it was not). In these cases, both forms of the variable may be preserved in the dataset as each will suit different needs of the users. Special attention must be made in cases where these multiple-choice variables are key variables. Any manipulations to one form of the variable as part of the anonymization process should be reflected on the other form or the other form should be removed.\n \n\n\n4.1.3.3 Free text variables\nFree text variables are unstructured responses to open-ended questions or requests to specify a response to ‘other’ category in multiple choice questions. These variables need to be reviewed because they can be prone to errors or contain sensitive or identifying information.\nIn general:\n\nFree text variables with direct identifiers should be removed from clean version of datasets that are not meant to have direct identifiers.\nFree text variables should be removed from anonymous versions of datasets that will be published on the MDL unless the Data Provider requests to keep the free text variables (i.e .because they would be useful to research and do not pose any additional disclosure risk).\n\n \n\n\n4.1.3.4 Free text responses to other in a multiple-choice question\nResponses to specify other in a multiple-choice question should be checked to make sure that the response category does not already exist. Consider the example where a respondent is asked how they spent the cash they received from UNHCR and they were provided with the options: food, rent, clothing, utilities, education, livelihood inputs and other. A respondent responded ‘other’ and when asked to specify what other items they spent they money on, the response was ‘rice’. In this example, the response should have been coded as ‘food’.\nThese issues need to be shared with the Data Provider, and the best step for dealing with them determined jointly between the Data Provider and the Data Curator. If it is decided to recode the categories, the following code can be used.\nCleaning up free text responses in other category to multiple choice variables that have existing category\n\n\nCode\n# Recode existing category\nyour_dataset$spent_cash_on_food[your_dataset$spent_cash_on_other_specify==\"Rice\"] <- \"1\"\n# Replace response to other as 0\nyour_dataset$spent_cash_on_other[your_dataset$spent_cash_on_other_specify==\"Rice\"] <- \"0\"\n# Remove free text specifying other\nyour_dataset$spent_cash_on_other_specify[your_dataset$spent_cash_on_other_specify==\"Rice\"<-NA\n\n# Recoding several \"other\" categories to existing category\nyour_dataset$main_category <- if_else(your_dataset$other_category %in% c(\"main_category_with_space\", \"main_category_with_typo\"),  \"main_category\", your_dataset$main_category)\n\n# For example:\nyour_dataset$roof_type <- if_else(your_dataset$roof_type_other %in% c(\"tarpaulin \", \"Tarpaulin\", \"tapuline\"),  \"tarpaulin\", your_dataset$roof_type)\n\n\n \n\n\n4.1.3.5 Date and time\nDate and time formats should be consistent with each other, never use different formats in the same dataset.  For maximum compatibility, formats should adhere to the standard ISO 86012 or dd-mm-YYYY (i.e. 7th of May of 2021 should be written as 07-05-2021) or YYYY-mm-dd which is the standard for some R packages (see below). In either case, the year should always be four digits and not abbreviated into two.\nFormatting dates in R\nFirst, you need to identify the format in which your dates are provided. You can edit the format parameter with the as.Date command (in base R) with the codes in the table below:\n\n\n\n\n\n\n\nCode\n# Convert from character to date format using as.Dates\nyour_dataset$dates <- c(\"May 27 1984\", \"July 7 2005\")\nyour_dataset$dates <- as.Date(your_dataset$dates, format = \"%B %d %Y\")\n# Result: [1] \"1984-05-27\" \"2005-07-07\"\n\n#Using the lubridate package, dates can be easily converted to date format with the following convention YYYY-mm-dd\n# Convert date from character to date format when the original date is stored as a character in YYYY-mm-dd.\nyour_dataset$date <- ymd(your_dataset$date)\n# Convert date from character to date format when the original date is stored as a character in mm-dd-YYYY\nyour_dataset$date <- mdy(your_dataset$date)\n\n\n \n\n\n\n4.1.4 Data completeness\nEnsure that there are no gaps or missing information in the data that can be avoided. This will include checking the number of observations against what is reported by the Data Provider and/or in the report as well as comparing the variables in the dataset against the questionnaire or data collection form to make sure that all the expected data is included. Sometimes datasets will include blank records (rows) or variables (columns). These can be removed if they are not meaningful (e.g. system generated variables or kobo metadata, erroneous blank records). They do not need to be removed if they are associated with a real record or variable.\nRemoving records (rows) and variables (columns) with all missing values in R\n\n\nCode\n# remove blank records\nyour_dataset <- your_dataset[ rowSums(is.na(your_dataset)) < ncol(your_dataset) ,]\n\n#You may also want to remove variables (columns) that have no responses or are blank. \n# remove blank variables\nyour_dataset <- your_dataset[ colSums(is.na(your_dataset)) < nrow(your_dataset) ,]\n\n# remove blank records using the janitor package\nyour_dataset %>%\n     remove_empty(which=\"rows\")\n# remove blank variables\nyour_dataset %>%\n     remove_empty(which=\"cols\")\n\n\n \n\n\n4.1.5 Duplicate records\nDatasets should not include any duplicate records. The simplest way to look for duplicates in a dataset is to identify the variable (e.g. unique ID) or combination of variables (e.g. name, age, sex, geographic location) that must be unique to each observation and then verify that there are no observations sharing the same values.\nLooking for duplicates in R\n\n\nCode\n#Given the dataset your_dataset, you can remove duplicates based of a single variable using the following code:\nyour_dataset[!duplicated(your_dataset$your_variable), ]\n\n#using dplyr:\nyour_dataset %>%\n   distinct(your_variable,keep_all=TRUE)\n\n#More elaborate step-by-step guide to find duplicates in your_dataset:\n# Create data frame with list of IDs and the number of times they occurred\nn_occur <- data.frame(table(your_dataset$id))\n# Identify which IDs occurred more than once\nn_occur[n_occur$Freq > 1,]\n# Return the list of IDs that occurred more than once\nyour_dataset[your_dataset$id %in% n_occur$Var1[n_occur$Freq > 1],]\n\n#To check for duplicates using a combination of variables, first create a unique variable based off of those variables and then use the same codes as above to check for duplicates.\n# Create a new variable (combo_var) that combines multiple variables\nyour_dataset$combo_var <- paste0(as.character(your_dataset$var_x),\"_\", as.character(your_dataset$var_y))\n\n\n \n\n\n4.1.6 Missing values\nDatasets and variables will often contain missing (or blank) values for several different reasons. It should be clear what exactly these values mean. For example, do they indicate that the question is non-applicable, is it a non-response, a true missing value or a value of 03. If it is not clear, the Data Provider should be consulted to clarify the meaning of the missing or blank values.\nWorking with missing values can be a bit tricky depending on how they appear in the dataset. Often, missing values are left blank, and R treats them as NA. Other times they may have specific codes defined during data collection (i.e. -9999, 9999, etc.). This should be recoded to missing as to ensure summary statistics are not being affected by the codification of these values.\nDealing with missing values in R\n\n\nCode\n#How data can be imported in R converting blank values to NA and replacing 0 and 9999 values with NA in cases they were incorrectly coded.\n\n# Import data from a csv file encoded in UTF-8. The values specified in na.strings will be set automatically as missing values for all variables\nyour_dataset <- read.csv(\"your_dataset_file_name.csv\", na.strings=c(\"\",\"NA\"), encoding = \"UTF-8\")\n\n# Replace all 0 and 9999 values with NA in cases they were incorrectly coded\nyour_dataset$your_variable[your_dataset$your_variable ==0] <- NA\nyour_dataset$your_variable[your_dataset$your_variable ==9999] <- NA\n\n#Note that R does not always show NAs in a frequency table. To force R to show the NAs, adapt the code as follows. Given your_dataset and your_variable:\n# Frequency table without NAs\ntable(your_dataset$your_variable)\n\n# Frequency table with NAs\ntable(your_dataset$your_variable, useNA = \"always\")\n\n#Finally, to export data replacing the NA with blank values, the following code can be used.\n\n# Save dataset as csv file, setting missing values to blank values\nwrite.csv(your_dataset, \"your_dataset_file_name_new.csv\", row.names = FALSE, na=\"\", fileEncoding = \"UTF-8\")\n\n\nThe proportion of allowed missing values (not to be confused with non-applicable, which is not a missing value) will depend on the impact on data quality and is something that is determined by the Data Curator in consultation with the Data Provider.\nCheck for missing values in R\n\n\nCode\nYou can perform common checks on missing values with the following code:\n# calculate the total proportion of missing observations in the dataset \nmean(is.na(your_dataset)) \n \n# calculate the proportions of missing observations for each variable in the dataset and show them in decreasing order \nsort(colMeans(is.na(your_dataset)), decreasing = TRUE) \n\n# create crosstab of two variables to see how missing values are distributed \ntable(your_dataset$your_variable1, your_dataset$your_variable2, useNA = \"always\")\n\n# Delete variables with all NAs\nyour_dataset =your_dataset[,colSums(is.na(your_dataset)) <nrow(your_dataset)] \n \n# Calculate the proportion of missing observations and create a chart showing most common missing value patterns using the VIM package\nsummary(aggr(your_dataset, sortVar=TRUE))$combinations\n\n#Finally the naniar package can be useful to visualize missing values. Read more here: https://www.rdocumentation.org/packages/naniar/versions/0.6.1 \n\n\n \n\n\n4.1.7 Excess spacing in string values\nString values should never start or end with a space or contain repeated spaces. This issue may be difficult to spot since empty spaces are invisible, but they cause problems during analysis since they are recognized by statistical software. For example, “your string” and ‘your  string’ may look the same when visually exploring the dataset, but when you create cross tabulations for example, they are different. It may be useful to check this early on because it can assist when checking consistent responses, for example.\nThe different possible cases are: \n\nLeading space: ‘your string’ \nTrailing space: ‘your string’ \nRepeated space between words: ‘your    string’ \nAny combination of the above: ‘your    string’ \n\nMost softwares have a trimming function to quickly remove trailing, leading and repeated spaces.\n\n\nCode\n#The package stringr provides a function called str_squish to trim all spaces in a string. \n\n# trim all the values in a variable\nyour_dataset$your_variable <- str_squish(your_dataset$your_variable)\n\n#The package janitor provides a function to remove all trailing spaces in variable names:\n\nyour_dataset <- clean_names(your_dataset)\n\n\n \n\n\n4.1.8 Consistent responses\nChecks should be done on the consistency in responses to key variables, other variables of interest to the Data Users, and/or any variables that the Data Provider advised should be checked. In some cases, inconsistencies may not be discovered until detailed analysis is performed, and the outputs seem ‘off’. In any case, the Data Curator should always take a step back and look at the dataset large to see if any obvious issues can at least be spotted. Below are some common examples.\n \n\n4.1.8.1 Inconsistent data categories\nData category inconsistencies occur when a variable has two or more distinct values representing the same category. This can be caused by synonymous words, typos, formatting, etc. This includes ensuring that, for example, responses to multiple choice questions fall into one of the valid answer options in the questionnaire or data collection form. \nThe following table highlights an example of inconsistent use of country names, Swaziland and Eswatini refer to the same country as does Côte d’Ivoire and Ivory Coast\n\n\n\nid\ncountry_origin\n\n\n\n\n1\nSwaziland\n\n\n2\nCôte d’Ivoire\n\n\n3\nEswatini\n\n\n4\nIvory Coast\n\n\n\nCheck for variable inconsistencies in R\n\n\nCode\n#You can identify inconsistencies and fix them with the following code:\n# Print all existing values for the variable to spot any issue \ncat(sort(unique(your_dataset$your_variable)), sep=\"\\n\")\n# Create a frequency table to see how many records fall in each category\ntable(your_dataset$your_variable)\n\n#If necessary, replace the values\nyour_dataset$your_variable[your_dataset$your_variable ==\"Ivory Coast\"] <- \"Côte d'Ivoire\"\nyour_dataset$your_variable[your_dataset$your_variable ==\"Swaziland\"] <- \"Eswatini\"\n\n\n \n\n\n4.1.8.2 Consistent use of capitalization in string values\nMost statistical software applications are case sensitive, meaning they consider differently two strings with different capitalization. See the example frequency table below where Bas Sassandra is repeated because one of the spellings has lowercase for the second word in the name of the district.\n\n\n# A tibble: 6 x 2\n  province      population\n  <chr>              <dbl>\n1 Abidjan               70\n2 Bas sassandra          2\n3 Bas Sassandra          7\n4 Cavally                4\n5 Guemon                 1\n6 Tonkpi                 7\n\n\nCheck and fix inconsistent capitalization in R\n\n\nCode\n#Consider the dataset your_dataset and the variable your_variable. A simple way to observe inconsistencies in value labels is to view a table of the responses in the variable.\n# Create a frequency table\ntable(your_dataset$your_variable)\n\n# Replace error with simple find and replace  \nyour_dataset[your_dataset$your_variable==\"your response\"] <- \"Your Response\"\n\n#Alternatively, you can capitalize all values of a variable in the same manner. This is particularly useful if there are many values to fix.\n# Change all to UPPER CASE\nyour_dataset$your_variable <- toupper(your_dataset$your_variable)\n# Change all to lower case\nyour_dataset$your_variable <- tolower(your_dataset$your_variable)\n# Change all to Title Case\nyour_dataset$your_variable <- tools::toTitleCase(tolower(your_dataset$your_variable))\n\n\n \n\n\n4.1.8.3 Data table alignment\nAnother cause for inconsistencies is a data entry mistake where variable values are inserted in the wrong column. For example, the values of the variable gender might have been pasted in the column corresponding to the age variable. If gender is a string variable (female/male) this would be easy to notice, but if the variable is numeric (1/2) it could be harder to spot. This mistake might affect several variables and multiple observations. Sometimes this can be flagged by simply observing the data frame. See an example below, where it seems at some point in the data manipulation process the data were shifted, as can be seen since the identifier variable (metainstanceID) is assigned to another column (XTOLPW) .\n\n\n\n\n\n \n\n\n\n4.1.9 Consistent variable relationships\nThe Data Curator should identify the main relationship between variables and check for their consistency. Some of the most common ones to look out for include:\n\nThe age of the respondent should be within the age range acceptable to respond to the interview, and what makes sense with the data (i.e. can a 2-year old child be working?).\nThe sum of members in a household broken down by age and gender should be equal to a variable on total household size\nThe number of pregnant and lactating women should not be larger than the number of women in the household\nGeographic locations should fall under the administrative units where they are located\nSome questions may only be relevant based on the answer to previous questions (i.e. only for recipients of a given programme, only for households with children, etc.)   \n\nIt is important to check and make sure the patterns are consistent, and if in doubt contact the Data Provider. Consider the following example:\n \n\n\n\n\n\n\n\n\n\n\n\nid\ncamp\nstatus\nprogramme\ncash_received\nparcels_received\n\n\n\n\n1\nOrange\nRefugee\nWFP food parcel\n250’000\n\n\n\n2\nOrange\nRefugee\nWFP food parcel\n\n1\n\n\n3\nPineapple\nRefugee\nWFP food parcel\n\n1\n\n\n4\nLemon\nIDP\nCBI\n250’000\n\n\n\n5\nPineapple\nRefugee\nCBI\n250’000\n\n\n\n6\nPineapple\nRefugee\nWFP food parcel\n\n1\n\n\n7\nLemon\nIDP\nCBI\n250\n\n\n\n\n \nThis table includes a couple of obvious errors and one suspicious issue highlighted in blue:\n\nLine 1 - The refugee household part of the WFP food parcel programme reported receiving cash. This is an obvious error, and the Data Provider should be consulted to see if they can confirm what the correct number of parcels should be or if the data on parcel_received will be a loss.\nLine 5 – The response here is suspicious because is the only household in Pineapple camp that was part of the CBI programme and the only refugee in the dataset that received cash (all others were IDPs). The Data Provider should be consulted just to confirm this is correct.\nLine 7 – The data on the amount of cash received appears to be an error when compared with all other data, most probably the Data Collector didn’t type out all the zeros. This should be confirmed with the Data Provider.\n\nIt will not be possible for a curator to fix and spot all inconsistencies within a dataset. Therefore, we recommend to select some variables in consultation with the Data Provider, that could have a large impact on utility and focus on those.\nCheck and fix inconsistencies in variable relationships in R\n\n\nCode\n#A simple way to observe variable relationships is to use crosstabs. This can be done very easily in Excel using a pivot table or in R using the table function.\n# create a crosstab \ntable(your_dataset$programme,your_dataset$cash_received)\ntable(your_dataset$programme,your_dataset$parcels_received)\n \n# Print the affected records \nyour_dataset[your_dataset$programme==\"WFP food parcel\" & !is.na(your_dataset$cash_received),]\n\n# Suppress incorrect value\nyour_dataset$cash_received[your_dataset$programme==\"WFP food parcel\" &\n                            !is.na(your_dataset$cash_received)] <- NA\n\n#Say the Data Provider confirmed that record 1 received 1 food parcel\n# Fill missing value\nyour_dataset$parcels_received[your_dataset$id==1] <- 1\n# Check work\nyour_dataset[1,]\n\n#Say the Data Provider confirmed that 250 reported by record 7 is a typo\n# Replace incorrect numeric value\nyour_dataset$cash_received[your_dataset$cash_received==250] <- 250000\n\n#Finally, say the Data Provider confirmed that line 5 is correct. As such, nothing needs to be done.\n\n\n \n\n\n4.1.10 Outliers\nOutliers are data points that differ significantly from other observations. They may be due to variability in the observation or may indicate an error. Using charts (histograms, scatter plots and box plots) and crosstabs is a good way to spot outliers. The treatment of outliers should be decided in consultation with the Data Provider.\nNote: Outliers may be treated differently in the cleaning and anonymization process. In the cleaning process, they should only be removed or modified if the value is wrong. Another approach may be to flag the outliers instead of fixing them. In the case that the outlier is in a key variable and is not wrong (i.e. a true outlier), then this record will likely increase the risk of disclosure of that particular Data Subject and may need to be treated to lessen the risk. This is addressed in the next chapter.\nConsider the simplified example below with three variables, id, income_source, income_1month\n\n\n# A tibble: 15 x 3\n      id income_source                income_1month\n   <dbl> <chr>                                <dbl>\n 1     1 Factory employee                     20000\n 2     2 Factory employee                      6000\n 3     3 Factory employee                      7000\n 4     4 Factory employee                      6000\n 5     5 Factory employee                      5000\n 6     6 Factory employee                      5000\n 7     7 Factory employee                      6000\n 8     8 Seller / Commercial activity         37000\n 9     9 Seller / Commercial activity         39000\n10    10 Seller / Commercial activity        400000\n11    11 Agriculture / Sale of crops           5000\n12    12 Agriculture / Sale of crops           4500\n13    13 Agriculture / Sale of crops           4000\n14    14 Agriculture / Sale of crops           4700\n15    15 Livestock / Sale of animals           7500\n\n\nAt first glance, it looks as if there may be a couple outliers, particularly record 1 and 10. The following outlines various ways to explore the outliers in R.\n\n\nCode\n#Using the example above, and for the dataset \"outliers\", simple way to start is to look at the summary of the variable\n\n# Summary of variable\nsummary(outliers$income_1month)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   4000    5000    6000   37113   13750  400000 \n\n\nCode\n# Summary of variable with a condition\nsummary(outliers$income_1month[outliers$income_source==\"Factory employee\"])\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5000    5500    6000    7857    6500   20000 \n\n\nCode\n# Calculate outlier cutoff of 1.5x the interquartile range \nreview_outliers <- 1.5*IQR(outliers$income_1month)\nprint(review_outliers)\n\n\n[1] 13125\n\n\nCode\n# Create a histogram of income_1month\nhist(outliers$income_1month, xlab=\"Income 1 month\", xlim=c(0, 1000))\n\n\n\n\n\nCode\n# Create a new variable for months in thousands to make it easier to read on chart and recreate histogram defining the number of breaks to ensure can see outliers clearly\noutliers$income_1month_thousands <- outliers$income_1month/1000\nhist(outliers$income_1month_thousands,xlab=\"Income 1 month in thousands\", breaks = 25)\n\n\n\n\n\nCode\n# Create a scatter plot of income_1month\nplot(outliers$id, outliers$income_1month_thousands, xlab=\"ID\", ylab=\"Income 1 month in thousands\")\n\n\n\n\n\nCode\n# Create boxplot of income_1month\nboxplot(outliers$income_1month_thousands, ylab=\"Income 1 month in thousands\") \n\n\n\n\n\nCode\n# Create boxplot of the income_1month variable split by income_source \nboxplot(income_1month_thousands~income_source, data = outliers)\n\n\n\n\n\n \n\n\n4.1.11 Consistency primary analysis / report\nIf an analytical piece or report has been produced with the dataset, consistency between the raw/clean version of the data and the analysis/report should be checked4. Sharing a dataset that differs with the report will later result in confusion and requests for clarification from the final users. The following is list of the minimum issues to check before moving on to the next stage of curation.\n\nThe total number of observations in the dataset is the same as reported in the analysis/report. \nThe main key indicators presented in the analysis/report can be correctly calculated from the dataset. \nMost of the variables used in the analysis/report or in the data collection form/questionnaire are in the dataset. Note that some variables may be missing if there were issues in their collection, however in general anything reported against should at least be present in the clean version of the dataset even if some need to be removed for the anonymous version (discussed in next chapter).\n\nAny discrepancies should be discussed with the Data Provider and documented in the metadata on RIDL.\n \n\n\n4.1.12 Consent\nIn some cases, the data will include a variable associated with a question on the “consent” of the Data Subject. The subject of what this “consent” entails will depend on the data exercise and should be available from the Data Provider. The following provides an overview of some of the typical cases, and how they should be treated AFTER consultation with the Data Provider:\n\nNormally if the data subject did not provide consent, the data will still include a record for that data subject but all variables after the consent variable will be left blank or NA. This can help facilitate calculating the response rate. If the record only includes whether or not they provided consent, and information related to the strata (i.e. camp, province, district, etc.), if applicable, and a unique ID or index, then the variable can be preserved.\nIn cases where a respondent did not provide consent and some personal information is still present, all this personal information must be removed.\nIn cases where the Data Provider advises that the entire record is removed, the entire record should be removed.\n\nRemoving data from records that did not provide consent in R\n\n\nCode\n# Remove information from your_variable for records when your_consent_variable is “No”\nyour_dataset$your_variable[your_dataset$your_consent_variable==\"No\"]<- NA\n\n#example of removing a record whose consent is “No” using dplyr\nyour_dataset <- your_dataset %>% filter(!your_consent_variable==\"No\")\n\n# Drop records with missing consent using dplyr\nyour_dataset <- your_dataset %>%\n  drop_na(your_consent_variable)\n\n# Keep only records where response to consent is 1 or NA using subset function\nyour_dataset <- subset(your_dataset,your_dataset$your_consent_variable == 1 | is.na(your_dataset$your_consent_variable))\n\n# Drop records with no consent which was reported as either \"No\" or \"No consent\" in this example using dplyr\nyour_dataset <- your_dataset %>%\n  filter(!your_consent_variable %in% c(\"No\", \"No consent\"))\n\n\n \n\n\n4.1.13 Unique ID\nEvery data table should have a unique ID, either auto-generated by the data collection software or manually generated by the Data Provider. If this is not already present, it should be created for the clean version of the data table so that reference can be made to the data table. A simple \\(1:n\\) can be created.\nCreate a unique ID\n\n\nCode\n# Assign a unique ID using numbers 1:n and place it at the beginning of the data table \nyour_dataset$unique_id <- 1:nrow(your_dataset) \nyour_dataset <- select(your_dataset, unique_id, everything())\n\n\nIn addition to the original unique ID, a pseudoID should be created that can be used to re-link the clean version of the data with the eventual anonymous version (because the original unique ID will be removed as part of the anonymization). Before it is necessary to randomize data rows because their original order may be used to guess some anonymized values. For example, household data may have been recorded by a particular geographical order, or the household members could have been interviewed from the oldest to the youngest.\nCreate pseudo unique ID\n\n\nCode\n# randomly shuffle data\nset.seed(42)\nrows <- sample(nrow(your_dataset))\nyour_dataset <- your_dataset[rows, ]\n\n# assign new id, ranging 1 to n, and position it before the original id\nyour_dataset$pseudo_unique_id <- 1:nrow(your_dataset) \nyour_dataset <- your_dataset %>% relocate(\"pseudo_unique_id\", .before = \"unique_id\")\n# Be mindful that this code will generate a new pseudo ID every time you run it. If you are removing observations at a later stage based on the new pseudo ID, you will have to adjust the code every time. \n\n\n \n\n\n4.1.14 Prepare weights\nTo calculate the risk of re-identification of data subjects, the weight of a given data subject needs to be adequately estimated. If the data are from a sample, for example, the weight of any given record is much larger than that from a full enumeration or census where every single data subject is found within the dataset. Design weights (also referred to as base weights or raw weights) in sample data are the number of units in a population that each unit in the sample represents (i.e. if each unit in a sample represents five units in the population, the design weight is 5).\nNote While weights may not be used for analysis of a self-weighted sample as each record equally represents its profile in the population of interest, they still need to be calculated for SDC to property calculate the risk of re-identification of data subjects in a dataset. This is because each record only represents a certain proportion of the overall population, which reduces the risk of re-identification as compared to a full enumeration or census.\nFor example, the weight of each record in a full enumeration or census would be equal to 1 (meaning the probability of selection is \\(1/1\\)) whereas the raw weight of each record in a simple random sample of 50 individuals out of a total of 200 individuals would be 4 (meaning the probability of selection is \\(1/4\\)).\nsdcMicro uses the universal weight 1 if no weight is provided, equating data to a full enumeration or census. As such, full enumeration or census data do not need weights before going into sdcMicro. If the microdata is from probabilistic sample surveys, each observation should have a corresponding weight, calculated by Data Provider based on the sampling methods used to design the study.  Unfortunately, weights are not always provided in the data, in which case they should be requested to the Data Provider. In the simplest sampling designs, weights can be added to sample data using the following formulas.\nIn sdcMicro, normalized weights cannot be used.\n \n\n4.1.14.1 Simple Random Sampling\nIn the case of simple random sampling, where each respondent has an equal probability of being included in the sample, the design weight is calculated for all records in the dataset using the following formula:\n\\[\n\\text{survey weight = }{\\frac{\\text{Total number of potential data subjects in sample frame}}{\\text{Total number of surveyed data subjects}}}\n\\]\n \nCreate sample weights in R for a sample frame of 6000 subjects\n\n\nCode\n# create design weight variable\nyour_dataset$weight <- 6000/nrow(your_dataset)\n\n\n \n\n\n4.1.14.2 Stratified Random Sampling\nIn a stratified random sampling, where the population of interest is divided in distinct strata and samples are drawn separately from each stratum, the design weight needs to be calculated for each stratum using the following formula:\n\\[\n\\text{survey weight = }{\\frac{\\text{Total number of pot. data subjects in strata 1’s sample frame}}{\\text{Total number of surveyed data subjects in strata 1}}}\n\\] \nAdding weights to a dataset that used stratified random sampling in R\n\n\nCode\n#The following is an example for creating a design weight variable called weight in your_dataset with two strata: camp1 has 300 households and camp2 has 700 households in the sample frame.\n\n# create design weight variable for each stratum\nyour_dataset <- your_dataset %>%\n  mutate(survey_weight = case_when(camp == \"camp1\"~ 1000/300,\n                            camp == \"camp2\"~ 1000/700))\n\n\n \n\n\n\n4.1.15 Save clean version and cleaning script\nIf any modifications/cleaning measures were taken in the previous steps, a clean version (or new clean version) of the file can be saved and uploaded to RIDL. If the initial data was the raw version, this should be saved as version 1.1. If the initial version was a clean version (e.g. version 1.1), this one should be stored as a new version (e.g. version 1.2). Additionally, the R script used to create this version should be saved and uploaded to RIDL with the clean version. This is most easily done in the r studio interface by going to File > Save As.\nExporting data in R\n\n\nCode\n#Using the haven package, you can export your_clean_data with the following code:\n# Export csv\nwrite.csv(your_clean_data,\"0_data/clean/your_clean_data.csv\",row.names=FALSE)\n\n#Export dta\nwrite.dta(your_clean_data,\"0_data/clean/your_clean_data.dta\", version = 11, label = attr(data, \"label\"))\n\n#Export RDS, R\"s data format\nsaveRDS(your_clean_data, \"0_data/clean/your_clean_data.RDS\")"
  },
  {
    "objectID": "anonymize.html#sec-5anon",
    "href": "anonymize.html#sec-5anon",
    "title": "4  Anonymize",
    "section": "4.2 Anonymization",
    "text": "4.2 Anonymization\nAnonymization is an iterative process involving three stages: assess the risk of re-identification, reduce the risk of re-identification (i.e., anonymize), and assess the utility of the data (or loss of information). While each stage is distinct in its method and purpose and the process always starts with assessing the risk, the process involves moving back and forth between the stages until the data has been effectively rendered anonymous, as shown in the figure below.\n\n\n\n\n\nUNHCR uses Statistical Disclosure Control (SDC) methods to assess and reduce the risk of re-identification of data subject in each dataset. This handbook does not include theory around anonymization, statistical disclosure control and other methods of minimizing disclosure risk as several available resources from the humanitarian and development field on the topic already exist. For theory, the two that are most often referred to by the Data Curation Team include:\n\nStatistical Disclosure Control for Microdata: Theory developed by the World Bank\nAn Introduction to Disclosure Risk Assessment developed by the UN OCHA Centre for Humanitarian Data\n\nEvery dataset has its own characteristics, so this guidance should be considered indicative only. Some cases will require more steps than others.\nThe main part of this section covers the classic example of a household or individual level microdataset(that is not hierarchical). Annex – Further guidance on anonymization provides more details on the following:\n\nHousehold surveys\nCash-based intervention (CBI) post-distribution monitoring (PDM) surveys\nproGres data\nSENS survey data\nWASH survey data\n\nThis section is written under the assumption that the data has already been checked for (see Section 4.1) and all the preliminary information needed for anonymization has been compiled see Compile Info\nAnonymization checklist\n\nRemove variables that will not be published\nPrepare sample (if relevant)\nAssess risk\nReduce risk\nAssess utility\nAdd/modify labels\nSave anonymous version and script\n\n \n\n4.2.1 Create new dataframe\nIf working continuously in r, create a new data frame labelled anonymous (e.g. data_anonymous) that is distinct from the clean version (e.g. data_clean) so that comparisons can be made later as part of the utility analysis.\nCreate a new dataframe in R\n\n\nCode\n#Create anonymous version\nyour_anonymous_data <- your_clean_data\n\n\n \n\n\n4.2.2 Remove variables that will not be published\nAny variables that were preserved with the clean version of the data but will not be released with the public version should be removed at this stage.\nVariables to be removed from publicly released version of data\n\nDirect identifiers – If some were kept in the clean data, such as unique IDs to link to other datasets, they should now be removed.\nSensitive variables - It may also be necessary to remove variables that are considered too sensitive for a release in a given context, for example, responses to questions about subjects of violence, religion or ethnicity. You should always consult with the Data Provider about the sensitivity of variables, always and especially when in doubt.\nKey variables that cannot be released - Sometimes, during the anonymization process, you may realize that it would not be possible to sufficiently anonymize certain key variables. For example, a key variable with a lot of variation such as the name of the camp/village in a dataset that includes over 20 different camps and villages. In this case, you may decide to try to go through the SDC process with this variable as a key variable, but eventually remove it if it is impossible to anonymize without too much utility loss. More about key variables later in this chapter.\nFree text variables - Free text variables usually are not useful for the final user analysis and may contain sensitive information. See Free text variables\n\nThe following code can be used to remove variables from a dataset.\n\n\nCode\n#This first example is a simple way to remove a single variable.\n# Remove a single variable from a dataframe\nyour_dataset$your_variable <- NULL\n\n#This second example is a simple way to remove multiple variables at the same time\n# Remove multiple variables from a dataframe\nyour_dataset [c(\"your_variable1\", \"your_variable2\", \"your_variable3\")] <- NULL\n\n#This third example is useful when developing a template to use for multiple datasets, because you can define a number of different identifiers and then R will search for them and remove them if they are found. You will not have any issues if they are not found. Say you want to identify and remove all direct identifiers\n# Identify direct identifiers\ndirect_identifiers <- c(\"enumerator_name\", \"address_household_number\", \"gpsCoordinates\", \"unhcr_progres_num\", \"telephone_num\") \n\n# Check if direct identifier variable exists and remove them\nfor (i in direct_identifiers){ \n if(i %in% colnames(your_dataset))\n{ your_dataset = your_dataset %>% select(-any_of(i))}\n}\n\n\n \n\n\n4.2.3 Prepare sample (if relevant)\nIf the data is census data (or a complete enumeration), it may be necessary to draw a sample of the data for release as opposed to the entire dataset. If the number of observations is small and the re-identification risk low, you can consider releasing the entire dataset. Otherwise, pull a random sample, preferably stratified to increase precision, from the dataset.\nExtracting a sample in R\n\n\nCode\n#This first example pulls a simple random sample of 20% from your_dataset.\n\n# get sample of identifiers \nid_var <- \"id\"\nsample_percentage <- 0.2\nsample_ids <- sample(your_dataset[[id_var]], size = round(sample_percentage * nrow(your_dataset)))\n \n# create weight and sample\nyour_dataset$weight <- nrow(your_dataset) / length(sample_ids)\nyour_dataset <- your_dataset[ your_dataset[[id_var]] %in% sample_ids ,]\n\n#This second example pulls a stratified simple random sample using the \"camp\" as strata. Note this is done using the dplyr package.\n\n# install the \"dplyr\" package if not already\ninstall.packages(\"dplyr\")\n \n#Draw a proportionately stratified sample, in this case 20%.\nset.seed(1)\nyour_dataset_sample = your_dataset %>%\n  group_by(camp) %>%\n  slice_sample(prop = 0.2)\n\n#Draw a sample where the same number of observation is pulled from each strata, in this case 50.\nset.seed(1)\nyour_dataset_sample = your_dataset %>%\n  group_by(camp) %>%\n  slice_sample(n = 50)\n\n\nNote that once the sample is drawn, the weights then need to be added. See Prepare weights.\n \n\n\n4.2.4 Assess risk\nThe risk of re-identification is assessed on the key variables5 and quantitatively analyzed using two main SDC indicators: \\(k\\)-anonymity and individual risk.\nk-anonymity\nA risk measure that is based on the calculation of the number of observations in a sample sharing the same combination of categorical key variables, where \\(k\\) is the number of data subjects sharing the same combination. An observation violates \\(k\\) - anonymity if the sample frequency count \\(fk\\) is smaller than the specified threshold \\(k\\) , while a dataset satisfies \\(k\\)-anonymity if no observations violate the specified \\(k\\)-anonymity threshold. The risk measure is the number of observations that violates \\(k\\)-anonymity for a certain value of \\(k\\), which can be calculated using sdcMicro.\nIndividual risk\nThe probability of correct re-identification of any of the observations in the dataset.\nRisk thresholds\nUNHCR”s thresholds for SDC for anonymizing the personal microdata of PoCs are presented in the table below.\n\n\n\n\n\n\n\n\nIndicator\nThreshold\nDescription\n\n\n\n\nk-anonymity\n3\nUnique combination of key variables is shared by at least three (3) observations in the data\n\n\nindividual risk\n<15%\nNo one observation has greater than 15% chance of re-identification\n\n\n\nCertain cases, outlined below, may require the thresholds to be adjusted. The final decision about how to treat these two cases rests with the Personal Data Controller, with support from the Data Provider, Data Protection Focal Point and Data Curator, and in consultation with the Chief DPO as necessary.\n\nFor certain personal microdata of PoCs, it may be determined that these thresholds need to be adjusted to allow for lower risk (i.e., in the case of sensitive microdata). If the risk of disclosure needs to be lower, \\(k\\)-anonymity could be higher and/or the individual risk threshold lower.\n\nFor personal microdata of PoCs derived from a full enumeration or census (i.e., not a sample), personal microdata that is not fully anonymized according to the thresholds listed above, or personal microdata that may be particularly sensitive or that contains sensitive data or information, UNHCR needs to either consider stricter SDC parameters (as outlined under point a) or a stricter Terms of Use (e.g., release data on the MDL as a Licensed Use File rather than a Public Use File).\n \n\n4.2.4.1 Define and prepare key variables\nSDC methods used in the anonymization process to reduce the risk of re-identifying data subjects focus on the analysis of key variables, namely factor (categorical) key variables or continuous variables that are converted to factor (categorical) as part of re-coding, and manipulations to the data focus on these key variables. Before diving into sdcMicro, it”s important to already know which variables will be key variables.\nKey variables are context specific, and therefore need to be identified for each dataset based on the possible disclosure scenarios. Annex - Key and sensitive variables  provides a more comprehensive list of common key and sensitive variables as well as appropriate anonymization techniques. Annex –  includes a list of key variables for specific datasets and surveys.\nOnly factor key variables are used as part of the risk assessment calculation. It is possible to input continuous (numeric) key variables into sdcMicro, however they are not used as part of the analysis of \\(k\\)-anonymity, individual or global risk. As such, discrete numeric variables or numeric variables with a finite number of responses such as age, household size, number of children in household, etc. should be treated as factor in sdcMicro. The following code provides an example of how to convert key variables to factor.\n\n\nCode\n#Convert categorical key variables to factor\nyour_dataset$your_variable <- as.factor(your_dataset$your_variable)\n\n\nAdditionally, only missing values coded as NA are read as ‘NA’ by sdcMicro so it is important that missing values in a key variable are coded as such and not, for example, 0, 99, etc. If this was not already done as part of the data check and preparation process, it should be done now.\n\n\nCode\n# Recode missing value code 99 to NA \nyour_dataset$your_variable[your_dataset$your_variable== 99] <- NA\n\n\n \n\n\n4.2.4.2 Define ghost variables\nVariables that might be linked to key variables are considered ‘ghost variables’ in SDC. Some examples below:\n\nthe name of a refugee camp is linked to the region where it is located;\nvariables associated with the disaggregation of total number of household members (e.g. by gender, age, etc.) are ghost variables of total number of household members; and\nincome per capita is a ghost variable of household size, etc.\n\nThe response to a ghost variable should be removed from the dataset if the response to their associated key variable is removed, and this key variable can be recalculated from the ghost variable.\nDefining ghost variables to be used by sdcMicro in R\n\n\nCode\n# Set up blank list that will be used to define ghost variables\nselectedGhostVars <- list()\n\n# Each linkage is a list, with the first element the key variable and the second element the linked variable(s)\nselectedGhostVars [[1]] <- list()\nselectedGhostVars [[1]][[1]] <- \"total_hh_size\"\nselectedGhostVars [[1]][[2]] <- c(\"hh_mem_0-17_yrs\",\"hh_mem_18_59_yrs\",\"hh_mem_60_yrs\")\n\n# If the dataset has more than one set of ghost variables, this can simply be added using the following\nselectedGhostVars [[1]] <- list()\nselectedGhostVars [[1]][[1]] <- \"total_hh_size\"\nselectedGhostVars [[1]][[2]] <- c(\"hh_mem_0-17_yrs\", \"hh_mem_18_59_yrs\",\n                                  \"hh_mem_60_yrs\")\nselectedGhostVars [[2]] <- list()\nselectedGhostVars [[2]][[1]] <- \"region\"\nselectedGhostVars [[2]][[2]] <- c(\"camp\")\n\n\n \n\n\n4.2.4.3 Create the SDC risk assessment (or SDC problem)\nTo initialize the SDC risk assessment (or SDC problem in sdcMicro), the SDC object (sdcObject) needs to first be created. The sdcObject encompasses all parameters that feed into the risk assessment. The following is a list of these parameters and sample code to create the sdcObject.\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ndat\nName of dataset (table) to go through SDC.\n\n\nkeyVars\nCategorical (factor) key variables\n\n\nnumVars\nNumeric key variables.\n\n\nghostVars\nGhost variables as described in previous section.\n\n\nweightVar\nWeight variable. If this is left blank, SDC assumes it is a census and not a sample dataset.\n\n\nhhID\nHierarchical identifier, if present. For example, if the table contains individual-level data that is linked to household-level data, then select the variable containing the unique ID for the household.\n\n\nstrataVar\nThe variable that defines the strata (either strata used for sampling or for analysis) used in perturbative methods like PRAM and Microaggregation. Don’t need to specify if it is not relevant.\n\n\npramVars\nVariables on which you want to apply the PRAM (post randomization method).\n\n\nexcludeVars\nVariables to be excluded from the SDC, and as a result dropped from the dataset. It is basically another way to remove variables from the dataset.\n\n\nseed\nThe seed makes it possible to reproduce the same results if you apply probabilistic anonymization methods. Please note that the seed should not be shared externally since it may make it possible to reverse the anonymization process. Always choose a random number, never use the default value.\n\n\nrandomizedRecords\nIf the records should be randomized. Can be TRUE or FALSE.\n\n\nalpha\nAlpha is the weight with which missing values contribute to the computation of key variables frequencies when calculating k-anonymity. Leave at default value of 1.\n\n\n\nSetting up an sdcObj in sdcMicro\n\n\nCode\n#The following is an example of setting up a sdcObject with ghost variables selectedGhostVars (see code in previous section) and strata variable camp and weight variable weight.\n\n# Set up sdcMicro object\nsdcObj <- createSdcObj(dat = your_dataset,\n                       keyVars = c(\"your_variable1\", \"your_variable2\"),\n                       ghostVars = selectedGhostVars,\n                       numVar = c(\"your_variable3\"),\n                       weightVar = c(\"weight\"),\n                       pramVars = NULL,\n                       hhId = NULL,\n                       strataVar = c(\"camp\"),\n                       excludeVars = NULL, \n                       seed = 346, \n                       randomizeRecords = FALSE, \n                       alpha=c(1))\n\n\nNote: In cases where the sdc parameter is not relevant, simply define it as empty or NULL as is shown in the above example. In parameters that call for a variable or a number of variables, the codes should be written as a list using the following syntax: c('variable_name','variable_name').\n\n\n4.2.4.4 Interpret the SDC risk assessment\nThe SDC risk assessment includes three different groups of quantitative information:\n\ninformation on the categorical key variables;\nindividual and global risk for categorical key variables; and\ninformation on k-anonymity.\n\n \n\n4.2.4.4.1 General overview and k-anonymity\nThe first step may be to get a general overview of the risk assessment, and k-anonymity.\n\n\nCode\n#Considering your sdcObj, the following code can be used to get a general overview of risk.\n\n# Summary of risk assessment results\nsdcObj\n\n\nExample SDC risk assessment output\nThe following is the SDC risk assessment from a household survey for a example dataset built-in the sdcMicro library. Note that this is an example of the problem BEFORE any modifications are made to the data. Reprinting the sdcObj after modifications are made will allow to compare the same information before the modifications and after. Each description is associated with the number in the screenshot of the R output.\n\n\nCode\nlibrary(sdcMicro)\n\ndata_sdcObj <- readRDS(\"data_sdcObj.RDS\")\n\n###Create object\nsdcObj <- createSdcObj(dat=data_sdcObj,\n                          keyVars=c(\"HH_SIZE\", \"MARITAL_STATUS\", \"REGION\"), \n                          weightVar=c(\"WEIGHT\"), \n                          hhId=NULL, \n                          strataVar=NULL, \n                          pramVars=NULL, \n                          excludeVars=NULL, \n                          seed=60, \n                          randomizeRecords=FALSE, \n                          alpha=c(1))\n###Risk assessment results\nsdcObj\n\n\nSection 1:\n\n\n\n\n\nBrief overview of the dataset that is part of the problem including number of records and variables, list of categorical key and numeric key variables and weight variable. If ghost variables were included, they would be listed here.\nSection 2:\n\n\n\n\n\nBasic description of the categorical key variables, including the number of categories going into the problem. Note that is example is showing unmodified data. If you run the code after the data is modified.\nSection 3:\n\n\n\n\n\nThe number of observations violating 2/3/5-anonymity. To interpret this, you can say that 49.45% of observations in the original dataset do not share the same responses to all categorical key variables with at least 2 other records.\n \n\n\n4.2.4.4.2 Individual and global risk\nCarefully examining global and individual risk can help to identify which observations are risky and further which key variable are causing observations to be more/less risky. You can start by printing out more details on the individual and global risk.\n\n\nCode\n#Individual and global risk in R\n#Considering your sdcObj, the following code can be used to print out details on individual and global risk.\n\n# Details on the global and individual risk\nsdcObj@risk\n\n#If you need further details, the following codes can help to view more about the specific risky observations.\n\n# Show the 10 largest individual risk measurements\nsdcObj@risk$individual %>% as.data.frame() %>% arrange(desc(risk)) %>% head(10)\n\n# List of observations violating 3-anonymity\nkanon3 <- your_dataset[sdcObj@risk$individual[,2] < 3,sdcObj@keyVars]\nkanon3 %>% print(n=Inf)\n\n#List of observations with individual risk > 0.15 or 15%\nindv15 <- your_dataset[sdcObj@risk$individual[,\"risk\"] > 0.15,sdcObj@keyVars]\nindv15 %>% print(n=Inf)\n\n\nThe result first show the measure of global risk followed by a table of individual risk for each observation. In the table on individual risk, there are three columns defined below:\n\n\\(f_{k}\\)= Sample frequency of the keys variables for all observations, whereas those with the same keys variables have the same frequency. If equal to 10, for example, there are 10 observations in the dataset with the same combination of key variables. If equal to \\(1\\), for example, this observation has a unique combination of key variables and is ‘sample unique.’ By definition, \\(f_{k}\\) is the same for all observations with a given key variable combination.\n\\(F_{k}\\) = Population frequency of a combination of key variables, which is the estimated number of respondents in the population with that number of similar combinations of key variables. If the microdata is a sample and not a census, \\(F_{k}\\) is the sum of the sample weights of all observations with the same key variable combination. Hence, like \\(f_{k}\\) is also the same for all observations with a given key variable combination.\nrisk or \\(r_{k}\\) = Individual risk or the probability of disclosure for the observations. As with \\(f_{k}\\) and \\(F_{k}\\), it is the same for all observations sharing the same pattern of values of key variables6\n\nAs the end goal is to have 0 observations violating 3-anonymity and 0% of records with individual risk >0.15 or 15%, it is helpful to print out a list of risky observation in R to understand if there are any trends around which key variables are increasing the level of risk.\n \n\n\n\n\n4.2.5 Reduce risk\nIf the risk assessment shows that the risk is not too high and k-anonymity is three (or the threshold set by the Data Provider and Personal Data Controller if different), then no manipulation needs to be done to the dataset. In this case, the Data Curator can go straight to section Save anonymous version, codebook and anonymization script.  \nIf the risk assessment shows that the risk is too high, then the data cannot be released without applying SDC methods.\nThe goal of SDC is to preserve as much of the data’s original content as possible, while reducing the risk of re-identifying data subjects.\nBear in mind that:\n1. Highly perturbative methods including PRAM, microaggregation, noise addition, shuffle and rank swapping should be avoided because they can highly distort the data based on the parameters used.\n2. Anonymization is an iterative process, which often requires starting over after attempting to anonymize a dataset once, twice, etc. and not being satisfied with the result and loss in utility. Using R with a save script helps to facilitate retracing your steps and also consult other Data Curators for advice.\nAn overview of anonymization methods is available in the table below. Complementary information can be found in the SDC Practice Guide. The technical part of this section focuses on the most used non-perturbative methods. For further support, reach out to microdata@unhcr.org.\n \n\n4.2.5.1 Summary of anonymization methods\n\n\n\n\n\n\n\n\n\nKey variable type\nMethod\nClassification\nExplanation and Example\n\n\n\n\nCategorical\nRecoding\nnon-perturbative, deterministic\nRecoding consists of grouping the values of categorical variables into a new variable. This is commonly used to regroup categorical variables with few responses into a larger group. For example, recoding responses with less than XX records to another like category or a group ‘other’.\n\n\n\nLocal suppression to achieve k-anonymity\nnon-perturbative, deterministic\nLocal suppression set the values of the key variables to missing (suppress) until the desired level of k-anonymity is reached (at least a 3-anonymity level).\n\n\n\nSuppression of linked (ghost) variables\nnon-perturbative, deterministic\nSuppressing ghost variables involves suppressing the response in a variable that is linked to the response in a key variable to missing if the response to the key variable is also suppressed (or set to missing). This step is necessary when there is a relationship between variables that could be used to reconstruct a suppressed value.\n\n\n\nSuppression of values with high risk\nnon-perturbative, deterministic\nThis method consists of suppressing (or setting to missing) the values of key variables for observations that surpass a certain risk threshold. It is most useful when you are aware of few records with high risk. The result depends on the selected key variable. You may need to repeat the suppression selecting other variables so that you remove the values of more than one key variable, or you may undo it and do it again selecting another key to see if you obtain a better result.\n\n\n\nPRAM (Post RAndomization Method)\nperturbative, probabilistic\nPRAM consists of randomly changing the values of categorical variables according to an invariant probability transition matrix. This method is especially useful when a dataset contains many key variables, and local suppression and recoding would lead to a significant information loss. One characteristic of the PRAM method implemented by sdcMicro is that univariate tabulations remain the same before and after anonymization for all variables that have been subjected to the method. This kind of implementation is called invariant PRAM. If a particular cross tabulation must be maintained, you can specify a strata argument.\n\n\nContinuous\nRounding\nperturbative, deterministic\nRounding is simply rounding a number up so that it is less precise. It is used to prevent exact matching of continuous key variables with external data sources. In addition, rounding can be used to reduce the level of detail in the data. For example, removing decimal figures of income variable or rounding income variable to the nearest thousand.\n\n\n\nTop/bottom recoding\nperturbative, deterministic\nThe purpose of top/bottom coding is to remove outliers (observations that lie at an abnormal distance from other values in a random sample from a population) from numeric key variables. Top and bottom coding are similar to recoding, but instead of recoding all values, only the top and/or bottom values of the distribution or categories are recoded. Top and bottom coding is especially useful if the bulk of the values lies in the center with only few observations outside (outliers). Some numeric values common to UNHCR data are related to age, expenditure and income, and usually you will have to top code high values, as there will often be a few observations above certain thresholds, typically at the tails of the distribution.\n\n\n\nMicroaggregation\nperturbative, probabilistic\nMicroaggregation is most suitable for continuous variables but can also be used for categorical. It works by grouping records that are homogenous with respect to the values of certain key variables. The values of selected variables are then replaced with a common value, e.g. the mean of that group.\n\n\n\nNoise addition\nperturbative, probabilistic\nIn noise addition, values are added to or subtracted from the original values of a variable. This prevents exact matching with an external file based on an exact value of a continuous variable.\n\n\n\nRank swapping\nPerturbative, probabilistic\nRank swapping is a type of data swapping used for ordinal continuous variables. Values of the variable are first ordered, and values are swapped with other values that are similar (in the same neighborhood).\n\n\n\nShuffle\nperturbative, probabilistic\nShuffling is similar to swapping but uses an underlying regression model to determine which variables are swapped.\n\n\n\nMany of the commonly used SDC methods can be done without using the sdcMicro package (i.e. using other R packages before entering into the sdcObj) or using specific sdcMicro functions inside the sdcObj. The Data Curator can choose the method most suitable, as long as utility analysis performed afterwards, and the methods are documented for transparency. The example codes below show examples both inside and outside of sdcMicro. Note that in the case that manipulations are performed outside of sdcMicro:\n\nThe risk analysis will only look at the final risk and not the true before and after.\nThe utility analysis in the sdcMicro generated report is not relevant because it is comparing a dataset that was already manipulated with the final anonymized version of the dataset. As such, the utility analysis will need to refer to the clean and anonymized data respectively.\n\n \n\n\n4.2.5.2 Categorical key variable methods\n\n4.2.5.2.1 Recoding\nRecoding key variables is a great way to reduce the percentage of records violating 3-anonymity without having to suppress too many values. At the same time, the following two points need to always be considered when recoding:\n\nRecoding should not cause too much loss of utility. For example, if indicators need to be calculated for children in school age (elementary and secondary), the recoding of this variable should be according to school year divisions, which would allow for calculations to be made i.e. 0-5, 6-11, 12-17. Instead, grouping in five-year intervals (0-5, 5-10, 10-15, 15-20) would greatly decrease the data utility for these users.  \nIf the variable can be recalculated using other variables, this anonymization method is not useful. A particular case is household size: in cases that there is an individual-level dataset, it can be recalculated simply by counting the number of individuals. Another example is, in the case of stratified sampling, the variable containing the strata (for example camp or camp zone). All observations in the same strata will have the same weight, so it would be possible to guess the strata just by looking at the unique weight value.\n\nRecoding variables using the sdcMicro package\n\n\nCode\n#Given your sdcObj and the key variables, province_district and total_hh_mem, the following code can be used to recode the variables.\n\n# Recode locations with few observations (Location 1, Location 4 and Location 5) into new group Other\nsdcObj <- groupAndRename(obj=sdcObj, var=\"province_district\", \n                        before=c(\"Location 1\",\"Location 4\",\"Location 5\"), \n                        after=c(\"Other\"))\n# Recode total number of household members into new distinct groups 1, 2, 3-4, 5-6, >6\nsdcObj <- groupAndRename(obj=sdcObj, var=\"total_hh_mem\", before=c(\"3\", \"4\"),\n                        after=c(\"3-4\"), addNA=FALSE)\nsdcObj <- groupAndRename(obj=sdcObj, var=\"total_hh_mem\", before=c(\"5\",\"6\"),\n                        after=c(\"5-6\"), addNA=FALSE)\nsdcObj <- groupAndRename(obj=sdcObj, var=\"total_hh_mem\", \n                        before=c(\"7\",\"8\",\"9\",\"12\"), after=c(\">6\"), addNA=FALSE)\n\n#See risk summary after recoding\nsdcObj \n\n#Recoding variables outside sdcMicro \n\n#Given the key variable age, there are a couple different methods to recode the age into groups. The first one is useful if the group value labels can be stored directly in the variable. The second one, using dplyr, is useful if the values are encoded with labels.\n\n# Recode age into 7 distinct groups method 1\nyour_dataset$age <- cut(your_dataset$age, \n                        breaks=c(0, 20, 25, 30, 35, 40, 45, Inf), \n                        labels=c(\"15-19\", \"20-24\", \"25-29\", \n                                 \"30-34\", \"35-39\", \"40-44\", \"45-49\"))\n\n# Recode age into 7 distinct groups method 2 using dplyr\nlibrary(dplyr)\nyour_dataset$age <- case_when(your_dataset$age %in% 0:19 ~ 1,\n                    your_dataset$age %in% 20:24 ~ 2,\n                    your_dataset$age %in% 25:29 ~ 3,\n                    your_dataset$age %in% 30:34 ~ 4,\n                    your_dataset$age %in% 35:39 ~ 5,\n                    your_dataset$age %in% 40:44 ~ 6,\n                    your_dataset$age %in% 45:49 ~ 7)\n# add labels to the codes\nval_labels(your_dataset$age) <- c(\"15-19\" = 1, \"20-24\" = 2, \"25-29\" = 3,\n                                  \"30-34\" = 4, \"35-39\" = 5, \"40-44\" = 6,\n                                  \"45-49\" = 7)\n\n#After this, the data can run through sdcMicro.\n\n\n \n\n\n4.2.5.2.2 Local suppression\nLocal suppression is used to suppress observations until \\(k\\)-anonymity is reached.\nIn cooperation with the Data Provider, key variables can be ranked by Data Curators in order of importance, meaning that some key variables will be preserved as much as possible compared to others during the anonymization process. So, for example, the location of a household may be more important than the gender of the head of household which may be considered more important than the age of the head of household. Please note however that using an importance vector will lead to a higher total number of suppressed values in the less important variables. Remember to indicate any ghost variables associated with a key variable before applying local suppression. See section Define ghost variables.\nLocal suppression in sdcMicro\n\n\nCode\n#The following code locally suppresses to reach 3-anonymity. The importance variables are coded according to the order in which they were listed in the sdcObj. So, for example, 1 is the first variable listed, 2 is the second, etc.\n\n# Local suppression to obtain 3-anonymity\nsdcObj <- kAnon(sdcObj, importance=c(1,2,8,3,4,7,6,5), combs=NULL, k=c(3))\n\n# See risk summary after local suppression\nsdcObj\n\n#localSupp function can be used to perform local suppression on a single key variable. This is particularly useful if the individual risk of certain records is still above the desired threshold (ie. 15%). \n# Local suppression to reach 15% individual risk\nsdcObj <- localSupp(sdcObj, threshold = 0.15, your_key_variable)\n\n# See risk summary after local suppression \nsdcObj\n\n# Undo the last local suppression – note this only undoes the last\nsdcObj = undolast(sdcObj)\n\n\n  #### Numeric key variable methods {#sec-numerickeyvar}\nNumeric key variables that cannot be considered factor are not used to calculate \\(k\\)-anonymity or individual risk. This increases the importance for direct communication with the Data Provider on the disclosure risk given the level of detail in numeric key variables. Two common methods for decreasing the level of detail in numeric key variables are rounding and top/bottom coding.\n \n\n\n4.2.5.2.3 Rounding\n\n\nCode\n# example using  sdcMicro\nsdcObj@manipKeyVars$your_variable <- round(sdcObj@manipKeyVars$your_variable)\n\n# example not using sdcMicro\nyour_dataset$your_variable <- round(your_dataset$your_variable)\n\n\n  ##### Top/bottom coding {#sec-5topcode}\nTop/bottom coding variables in R and sdcMicro\n\n\nCode\n# example using  sdcMicro\nsdcObj <- topBotCoding(sdcObj, value=500000, \n                       replacement=1000, column=\"your_variable\")\n# example not using sdcMicro\nyour_dataset$your_variable[your_dataset$your_variable >= 2] <- 2\n\n\n \n\n\n\n4.2.5.3 Removing records\nIn some cases, entire records may need to be suppressed either because they are too risky (e.g. the individual risk for a particular record is very high) or they are linked to another variable that can be used to re-calculate a key variable that was manipulated (i.e. suppressed, top coded, etc.). Consider the following two examples:\n\nIn a hierarchical dataset that includes two tables (one with the household at the subject and the other with the household members as the data subject). If all household members are interviewed, the household size can be calculated from the household member table. In this case, if there are manipulations to a variable that provides the size of the household in the household table, the responses to the variable can be recalculated based on the household member table by simply counting the number of members per household. One option to manage this situation, you could firstly top code the household size in the household data table, then randomly remove individuals from the household member table whose households exceed the threshold.\nData were gathered using stratified sampling and the strata correspond with different refugee camps and all observations in the same camp have the same survey weight. If the some of the observations in the camp variable are removed through local suppression, it is still easy to guess the camp through the survey weight. Since the survey weight cannot and should not be removed, the first solution would be to prioritize the camp variable when performing local suppression (see Local suppression) and the second solution would be to remove the records where the camp was suppressed from the anonymous version of the data table (if avoiding suppression is not possible).\n\nNote that suppressing records is not possible using any functions of the sdcMicro package.\nSupressing records in R\n\n\nCode\n#Given key variable camp:\n\n#example of removing records outside of sdcMicro where the variable camp is blank\nyour_dataset <- your_dataset[!is.na(your_dataset$camp),]\n\n#example of removing row number 134 outside of sdcMicro\nyour_dataset <- your_dataset[-c(134),]\n\n#example of removing a record whose hh_id is \"ABC\" using dplyr package\nyour_dataset <- your_dataset %>% filter(!hh_id==\"ABC\")\n\n# Example of removing records using subset function with two conditions\nyour_dataset <- subset(your_dataset,your_dataset$your_variable == \"ABC\" | your_dataset$your_variable==\"DEF\")\n\n\n \n\n\n4.2.5.4 Reassess risk\nAfter performing relevant SDC methods, the Data Curator re-evaluates the disclosure risk on the anonymized data using the same methods described in section Interpret the SDC risk assessment. The sdcObject summary will display k-anonymity before and after any manipulations that were done inside of the sdcObject. As previously stated, it is important to remember that if any manipulations were done outside of the sdcObject, they will not be factored in this analysis.\n \n\n\n4.2.5.5 Exporting data from SDC object\nOnce you are satisfied with an anonymized version of the data, it needs to be exported out of the sdcObject to assess utility and for the last fixes before release. This can be done using the following code.\nYou can export the anonymized version of the data, your_anonymous_data, using the following\n\n\nCode\n# export anonymized data file and save to disk (extract sdcObj)\nyour_anonymous_data <- extractManipData(sdcObj)\n\n\n \n\n\n\n4.2.6 Assess utility and utility loss\nOnce the risk thresholds are met, a comparison between the clean and anonymous version needs to be made focusing on re-calculating the main indicators and cross-tabulations of interest for analysis and research. If the difference is statistically significant, the SDC method will need to be repeated until the utility is preserved. If the utility cannot be preserved due to information loss in the anonymization process, then the microdata may not be relevant for external publication on the MDL.\nThe focus here is on those variables that may have been modified, e.g. the key variables. In datasets where entire records were suppressed, all variables in the dataset were modified and any of potential interest to Data Users need to be reviewed.\nUtility checks greatly depend on the kind of dataset and the needs of the final user. Common utility checks are:\n\nComparing published reports - If reports based on the dataset have been published, you should recalculate the main statistics on the anonymized dataset and verify that very similar results are obtained.\nCategorical key variables and variables of interest - Compare the distribution of the original dataset against the anonymized one. To establish if any differences are statistically significant, you can compare the confidence intervals on the proportions.\nContinuous key variables and variables of interest – Compare the same descriptive statistics on both the original dataset and the anonymized. For example, compare the means and standard deviations and any other statistics that the end user may need. The aim is to have no statistically significant differences between the dataset before and after the anonymization.\nRelationships between variables - From the report or information shared by the Data Provider, important relationships between variables may have been identified. In this case, it should be established that the relationship between these variables has been maintained.\n\nWhen assessing utility, it is important to compare the anonymous version of the data against the clean version of the data. In some cases, modifications as part of the anonymization process may have been done before the use of sdcMicro. As such, the initial data coming into the sdcObject are not the dataset that should be used for comparison but the version before any manipulations were made.\n \n\n4.2.6.1 Categorical variables\n\n4.2.6.1.1 Basic descriptive statistics\n\n\nCode\n#Descriptive statistics of categorical variables in R\n# Compare plots\nplot(your_clean_data$your_variable) \nmtext(\"Distribution before Anonymization\", side = 3, line = 1, cex = 1.2)\nplot(your_anonymous_data$your_variable)\nmtext(\"Distribution after Anonymization\", side = 3, line = 1, cex = 1.2)\n\n# Compare table distribution\ntable_clean <- table(your_clean_data$your_variable_1, your_clean_data$your_variable_2,useNA= \"always\") \nround(prop.table(table_clean, 1), 2)\ntable_anon <- table(your_anonymous_data$your_variable_1, \n               your_anonymous_data$your_variable_2,useNA= \"always\") \nround(prop.table(table_anon, 1), 2)\n\n# Calculate relationship between two categorical variables using chi-squared test \nchisq.test(table(your_clean_data$your_variable), table(your_anonymous_data$your_variable))\nprop.test(table(your_clean_data$your_variable), table(your_anonymous_data$your_variable))\n\n#The null hypothesis is that there is no difference between the distribution of the variable in the clean version and the anonymous version of the dataset. If the p-value of the test is above the threshold chosen for statistical significance (usually 0.10, 0.05, or 0.01 level), then the null hypothesis cannot be rejected meaning the difference is not statistically significant (what is desired in the utility analysis).\n\n#In summary, the p-value should be greater than 0.10 or 0.05 to establish that the difference between the clean and anonymous version of the variable is not statistically significant.\n\n\n \n\n\n\n4.2.6.2 Continuous variables\n\n4.2.6.2.1 Basic descriptive statistics\n\n\nCode\n# Compare means, covariance and correlations\nsummary(your_clean_data$your_variable)\nsummary(your_anonymous_data$your_variable)\nmean(your_clean_data$your_variable)\nmean(your_anonymous_data$your_variable)\ncov(your_clean_data$your_variable)\ncor(your_anonymous_data$your_variable)\ncov(your_clean_data$your_variable)\ncor(your_anonymous_data$your_variable)\n \n# Compare histograms \nhist(your_clean_data$your_variable, \n     main = \"Histogram: Your variable name - original clean data\")\nhist(your_anonymous_data$your_variable, \n     main = \"Histogram: Your variable name - anonymous data\")\n\n# Compare density distribution\nplot(density(your_clean_data$your_variable), \n     xlim = c(x, x), ylim = c(x, x), \n     main = \"Density\", xlab = \"Your variable\")\npar (new = TRUE) # adds the second graph on top\nplot(density(your_anonymous_data$your_variable), \n     xlim = c(x, x), ylim = c(x, x), \n     main = \"Density\", xlab = \"Your variable name\")\n\n\n \n\n\n4.2.6.2.2 Difference of means test\nA difference of means test is useful for comparing the significance of the difference of the mean of two continuous variables.\n\n\nCode\ndif_means <- t.test(your_clean_data$your_variable, your_anonymous_data$your_variable, var.equal = TRUE)\ndif_means #print the result\n\n#If the p-value is above the threshold chosen for statistical significance (usually 0.10, 0.05, or 0.01 level), then the difference between the means not statistically significant. \n\n\nThe following example compares the total number of household members in the clean version (dat_clean) of the dataset and anonymized version (dat_anon). The household size was topcoded as part of the anonymization process. The results of the t-test demonstrate that the difference in mean is not statistically significant. In summary, the p-value should be greater than 0.10 or 0.05 to establish that the difference between the clean and anonymous version of the variable is not statistically significant and the utility has been preserved.\n\n\n\n\n\n \n\n\n\n4.2.6.3 Variable relationships\n\n4.2.6.3.1 Fitted models\nRegression parameters of the same regression done on the clean and anonymous versions of the dataset can be done verify if the change was statistically significant. The choice of the regression formula depends on the type of dataset and the relationships between variables. For example, a regression could be used to explain earnings as a function of education and experience. If the new estimated coefficients fall within the original confidence interval, the data can be considered valid for this type of regression after anonymization.\n\n\nCode\n#Define the variables to use in the regression\ndependent_var <- \"your_dependent_variable\"\nindependent_var1 <- \"your_independent_variable_1\"\nindependent_var2 <- \"your_independent_variable_2\"\nindependent_var3 <- \"your_independent_variable_3\"\n\n#Define the weight variable\nweight_var <- \"your_weight_variable\"\n\n#Define the regression formula\nindependent_vars_formula <- paste(independent_var1, independent_var2,\n                            independent_var3, sep = \"+\")\nregression_formula <- paste(dependent_var, independent_vars_formula, sep = \"~\")\n#Note: the formula will be: \n#          your_dependent_variable ~ your_independent_variable_1 +\n#          your_independent_variable_2 + your_independent_variable_3\n\n#Calculate the regression of the original data\nregression_clean_dataset <- lm(regression_formula,\n                               your_clean_data,\n                               na.action = na.exclude,\n                               weights = your_clean_data[[weight_var]])\n#Calculate the regression of the anonymous data\nregression_anonymous_dataset <- lm(regression_formula,\n                               your_anonymous_data,\n                               na.action = na.exclude,\n                               weights = your_anonymous_data[[weight_var]])\n\n# Print the 95% CI for the clean dataset\nconfint(obj = regression_clean_dataset, level = 0.95)\n\n# this will print something like this:\n#                              2.5 %    97.5 %\n# (Intercept)                 1.0951    1.1437\n# your_independent_variable_1      -1.9614   -1.5778\n# your_independent_variable_2       3.3923    4.2352\n# your_independent_variable_3       2.5120    3.3995\n\n# Print the coefficients for the anonymized dataset\nregression_anonymous_dataset$coefficients\n\n# this will print something like this:\n#         (Intercept)          your_independent_variable_1 \n#              1.1074                        -1.7676\n#     your_independent_variable_2    your_independent_variable_3 \n#                    3.6787                   2.8088 \n\n#As the coefficients fall within the confidence interval for all three variables (from the original dataset), the relationship between variables is maintained after the anonymization. \n\n\n \n\n\n\n4.2.6.4 Applying survey weights to analysis\nIn cases of disproportionately stratified datasets, weights must be applied to analysis that spans across strata. The survey package can be used to apply the weights and run simple statistics.\n\n\nCode\n#Once the survey package is installed and loaded, the following can be used to create a new data frame your_dataset_survey that applies the survey weights (variable survey_weight) based on your_strata variable.\n\n# Create survey\nyour_dataset_survey <- svydesign(ids=~0, \n                                 weights=~survey_weight,strata=~your_strat,\n                                 survey.lonely.psu=\"adjust\",data=your_dataset)\n\n#Now you can use your_dataset_survey to calculate the statistics needed. Note that it is more useful to compare proportions produced by the results as opposed to summaries because the summaries will be estimates for the entire population of interest in the sample frame not just within the sample\n\n\n \n\n\n\n4.2.7 Save anonymous version, codebook and anonymization script\nData should be saved at least as a csv for publication on the MDL. If the raw/clean version were provided as a dta (stata) or sav (spss) that preserves additional details such as variable and value labels it should also be saved in those formats.\n\n\nCode\n#The following uses the haven package, so make sure it is installed and loaded before running the code below\n\n# Export csv\nwrite.csv(your_anonymous_data, \n          \"0_data/anonymous/your_anonymous_data.xlsx\",\n          sheetName=\"data\",row.names=FALSE)\n\n# Export xlsx\nwrite_xlsx(your_anonymous_data, \"0_data/anonymous/your_anonymous_data.xlsx\")\n\n\n# Export dta\nhaven::write_dta(your_anonymous_data, \"0_data/anonymous/your_anonymous_data.dta\")\n\n\nFor datasets with encoded values (responses), it can be very helpful for the Data Users to have a codebook that can be shared with the dataset. The example below demonstrates how to create a simple codebook. Note that this assumes that labels have already been created for the variables and values (see previous section).\nCreating a codebook in R\n\n\nCode\n# variable names and labels\nyour_dataset_var_codebook <- your_dataset %>%\n  var_label() %>%\n  enframe(name = \"Code\",\n          value = \"Label\") %>%\n  unnest(cols = Label)\n\n#  value names and labels\nyour_dataset_val_codebook <- map(select_if(your_dataset, is.labelled), function(col) {\n  distinct(tibble(values = as.character(col),\n                  labels = as.character(to_factor(col))))}) %>%\n  enframe(name = \"variable\") %>%\n  unnest()\n\n# export codebook to excel with a tab for variables and a tab for values\nwrite_xlsx(list(variables=\n                  your_dataset_var_codebook,\n                values=your_dataset_val_codebook), \n           \"your_dataset_name_codebook.xlsx\")\n\n\nFinally, the anonymization script and workspace (.R) should be saved to be stored with the rest of the curation files on the GDS SharePoint space. These files should not be shared on RIDL. The anonymization script can be saved using the R studio interface. The workspace can be saved using the example code below\n\n\nCode\n#save the workspace\nsave.image(file = \"1_scripts/your_anonymization_workspace.RData\")"
  },
  {
    "objectID": "drr.html#sec-6markdownformat",
    "href": "drr.html#sec-6markdownformat",
    "title": "5  Disclosure risk assessment",
    "section": "5.1 R Markdown text formatting",
    "text": "5.1 R Markdown text formatting\nThe following table provides some of the syntax used in R Markdown for formatting text in the DRA report. Note this is only a sample of the various types of formatting that can be done in R Markdown. The following cheat sheet is a useful tool to navigating all additional features:\n\n\n\nSyntax\nDescription\n\n\n\n\n# Header 1\nHeader 1\n\n\n## Header 2\nHeader 2\n\n\n### Header 3\nHeader 3\n\n\n### Header 4\nHeader 4\n\n\n** bold ** and __ bold __\nbold and bold\n\n\n* italics * and _ italics _\nitalics and italics\n\n\n`code text`\ncode text\n\n\n* unordered list\n·   unordered list\n\n\n+ sub-item\nsub-item\n\n\n1.     ordered list\n1. ordered list\n\n\n2.     ordered list\n2. ordered list"
  },
  {
    "objectID": "drr.html#sec-6gettingstarted",
    "href": "drr.html#sec-6gettingstarted",
    "title": "5  Disclosure risk assessment",
    "section": "5.2 Getting started",
    "text": "5.2 Getting started\nThe template used below will call on the anonymization workspace and pull any data frames and other objects created. Ensure the workspace was saved prior to starting to write the report (see Prepare weights).\nIn cases of disproportionately stratified datasets, weights must be applied to analysis that spans across strata. The survey package can be used to apply the weights and run simple statistics.\n\n\nCode\n# Create survey\nyour_dataset_survey <- svydesign(ids=~0, weights=~survey_weight,strata=~your_strata,survey.lonely.psu=\"adjust\",data=your_dataset)\n\n\nNow you can use your_dataset_survey to calculate the statistics needed. Note that it is more useful to compare proportions produced by the results as opposed to summaries because the summaries will be estimates for the entire population of interest in the sample frame not just within the sample\nTo start the report, open R studio and create a R Markdown file. Choose the HTML format and select the location to store the document.\nEvery R Markdown file starts with a YAML header, where the details of the document including title, date, author and template are specified. Details are always between - - -. The title and date will display on the first page of the report as well as in the header. It is defined from the beginning together with the report template.\nCreating the title and defining the template in R Markdown\n\n\nCode\n---\ntitle: 'UNHCR Microdata Disclosure Risk Assessment Report'\nauthor: 'First and Last Name - Position'\ndate: '`r Sys.Date()`’\noutput: unhcrdown::paged_simple\n---\n\n\nTo be able to “call” on the anonymization script and pull any data frames and other objects created, the anonymization workspace needs to be loaded as well as the packages used. The following code is an example. Note that is it in a code chunk that will be hidden from the report.\nAbout code chunks: Every code chunk should go between {r}. If the code output should be displayed but not the code itself, the option {r, echo = FALSE} is used. If neither the response nor the code should be displayed, the option {r, include = FALSE} should be used. More examples will be provided below.\nSetting up the workspace in R Markdown\n\n\nCode\n{r files, include=FALSE}\n# Load the working space \nload( “1_scripts/your_workspace_name.RData\"))"
  },
  {
    "objectID": "drr.html#sec-6report",
    "href": "drr.html#sec-6report",
    "title": "5  Disclosure risk assessment",
    "section": "5.3 The Report",
    "text": "5.3 The Report\n\n5.3.1 Introduction\nThe introduction should include the name of the dataset and the link to the dataset on RIDL. This can be written directly in R Markdown using, i.e. does not need to be part of a code chunk. Consider the following example.\n\n\nCode\n# Introduction\n\n__Dataset:__ \"Côte d'Ivoire: Mapping of persons at risk of statelessness - April 2019\"\n\n__RIDL link:__ <https://ridl.unhcr.org/dataset/civ-statelessness-april-2019/>\n\n\n\n\n5.3.2 Summary\nThe objective of the summary is to provide the main conclusions of the DRA. It should be one sentence or a short paragraph that includes the following information:\n\nif microdata could be rendered anonymous while preserving their utility; and\nif it is recommended to publish on the MDL and under which Terms of Use.\n\nThis can be written directly in R Markdown as a text editor:\n\n\nCode\n# Summary\n\n__These data could be anonymized while preserving their utility, and it is recommended that they be released as a licensed file on the Microdata Library.__\n\n\n\n\n5.3.3 List of roles\nThe list of roles should include the names of everyone involved in the process including the Personal Data Controller, Data Provider(s), Data Curator and Data Protection Focal Point. This can be written directly in R Markdown using a table format for readability. Consider the following example.\n\n\nCode\nData Curation Roles                   | \n------------------------------------- | -------------\nPersonal Data Controller              | Name (email)\nData Provider                         | Name (email)\nData Curator                          | Name (email)\n\n\n\n\n5.3.4 Summary of disclosure risk scenarios\nThis section responds to question 1 in the DRA checklist: What are the potential disclosure risk scenarios – e.g., realistic assumptions about who may be interested in the microdata and for what purpose – and available data and information (both internal and external) covering the same population group that could be linked to this personal microdata (e.g., through the mosaic effect)?\nIt should be a short paragraph or two, and can be typed directly using R Markdown as a text editor. Consider the following example.\n\n\nCode\n# Potential Disclosure Risk Scenarios\n\nAn intruder could be interested in identifying displaced individuals or households living in the camps or finding information about them that are observable such as specific household demographics, livelihood activities, living conditions, displacement experience and intentions for the immediate future.\n\nThe likelihood of re-identification of data subjects based on this dataset alone is low due to the fact that:\na) the anonymized dataset only includes a sample of the original full dataset,\nb) observable variables were checked for granularity and recoded where the level of detail would increase risk of re-identification, and\nc) the data was anonymized until 3-anonymity was reached.\n\nThe likelihood of re-identification of data subjects based on combining the anonymized version of the data with another publicly available dataset was not statistically measured, however is considered low due to the low level of publicly available on the data subjects.\n\n\n\n\n5.3.5 Anonymization methods\nThe anonymization methods section should include at minimum a description of the following:\n\nweights: an example of a section of weights could look like this:\n\n\n\nCode\n## Weights\n#Survey weights were calculated by dividing the total number of data subjects in the sample frame (N) by the number of data subjects in the sample (n) separately for each strata. The following table provides an overview:\n\nstrata <- c(\"Camp 1\", \"Camp 2\", \"Camp 3\")\nN <- c(500, 230, 475)\nn <- c(140, 50, 125)\nw <- c(3.5714,4.6000, 3.8000)\nweight_table <- data.frame(strata,N,n,w)\n\ncolnames(weight_table) <- c(\"\", \"N\", \"n\",\"weight\")\nlibrary(kableExtra)\n\n\nWarning: package 'kableExtra' was built under R version 4.1.3\n\n\nCode\nkbl(weight_table) %>%\n  kable_classic()\n\n\n\n\n \n  \n     \n    N \n    n \n    weight \n  \n \n\n  \n    Camp 1 \n    500 \n    140 \n    3.5714 \n  \n  \n    Camp 2 \n    230 \n    50 \n    4.6000 \n  \n  \n    Camp 3 \n    475 \n    125 \n    3.8000 \n  \n\n\n\n\n\n \n\nvariables removed\n\nIf any variables were removed from the dataset that will be released on the MDL, they should be listed here. If they were removed using a list (see example under Remove variables that will not be published), then it can simply be called and displayed. Consider the following example.\n\n\nCode\n## Variable removed\nThe following *direct identifiers* were removed from the dataset:\n\nprint(direct_identifiers)\n\n\n \n\nkey variables\n\nAll key variables that were used as part of the SDC risk assessment should be listed here. If other variables were modified, but not used as part of the SDC risk assessment, they can be listed under a separate header (e.g. Variables modified). Consider the following example.\n\n\nCode\n## Key variables\nThe following *key variables* were used for the SDC risk assessment:\n\n  {r, echo=FALSE}\nprint(names(sdcObj@origData[,sdcObj@keyVars]))\n\n\n \n\nSDC methods performed\n\nSome of this information can be written directly in R Markdown as a text editors; others can be pulled in from the anonymization workspace as code chunks. Consider the following examples broken down by section."
  },
  {
    "objectID": "validation.html",
    "href": "validation.html",
    "title": "6  Validation and authorization",
    "section": "",
    "text": "Before data can be published on the MDL, there are several steps that need to be taken, including, and most importantly ensuring that the Data Provider agrees with the curation, and validation and authorization is provided by the Personal Data Controller. The steps involved are outlined in section 5.6 of the Curation AI.\nQuarterly release Microdata are published on the MDL on a quarterly basis to limit the burden on the DIMAs and Personal Data Controllers to respond to validation and authorization requests particularly if they may have many throughout the year. That said, if requested, microdata can be published outside of the scheduled release."
  },
  {
    "objectID": "publication.html#sec-7metadata",
    "href": "publication.html#sec-7metadata",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.1 Create MDL metadata",
    "text": "7.1 Create MDL metadata\nThe metadata needs to be created for the MDL. See [Annex - Metadata guidance] for guidance on creating the metadata."
  },
  {
    "objectID": "publication.html#sec-7review",
    "href": "publication.html#sec-7review",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.2 Request another Data Curator to review metadata",
    "text": "7.2 Request another Data Curator to review metadata\nBefore publication, the team of Data Curators share the task of reviewing datasets uploading to MDL from other Data Curators. This is assigned 1-2 weeks before the quarterly publication."
  },
  {
    "objectID": "publication.html#sec-7publish-data",
    "href": "publication.html#sec-7publish-data",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.3 Publish data",
    "text": "7.3 Publish data\nData are published on a quarterly basis with releases in January, April, July and October."
  },
  {
    "objectID": "publication.html#sec-7update-ridl",
    "href": "publication.html#sec-7update-ridl",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.4 Update RIDL",
    "text": "7.4 Update RIDL\nUpdate RIDL with clean version of data, cleaning script and anonymous version of the data ensuring that each file is labelled correctly. See section Getting started for more details."
  },
  {
    "objectID": "publication.html#sec-7update-data-registry",
    "href": "publication.html#sec-7update-data-registry",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.5 Update data registry",
    "text": "7.5 Update data registry\nIf the operation or region associated with the dataset has a registry, ensure to update this inventory with the relevant information related to the curation. In addition, update any global data inventories that need to be updated."
  },
  {
    "objectID": "publication.html#sec-7update-sharepoint",
    "href": "publication.html#sec-7update-sharepoint",
    "title": "7  Publication on the MDL and other post-curation tasks",
    "section": "7.6 Update SharePoint",
    "text": "7.6 Update SharePoint\nSave a copy of all curation files on the following SharePoint space GDS – Data Curation > Data repository > region folder > country folder."
  },
  {
    "objectID": "annex1.html",
    "href": "annex1.html",
    "title": "Appendix A — Annex: R Packages",
    "section": "",
    "text": "The following is a list of R packages that can be useful to the work of a Data Curator, and a description of how they may be used. It’s obviously non-exhaustive.\n\n\n\n\n\n\n\nPackage\nDescription and use\n\n\n\n\ndplyr\nSeveral tools to manipulate and clean data. Part of the larger tidyverse package.\n\n\nforeign\nRead and write data stored by some versions of ‘Epi Info’, ‘Minitab’, ‘S’, ‘SAS’, ‘SPSS’, ‘Stata’, ‘Systat’, ‘Weka’, and for reading and writing some ‘dBase’ files.\n\n\nggplot2\nTools to create graphical representations of data. Part of the larger tidyverse package.\n\n\nhaven\nRead and write data stored in formats from other statistical pacakages including SAS, SPSS and Stata.\n\n\nhere\nConstructs paths to project files, avoiding issues with relative paths.\n\n\nkableExtra\nGenerate simple tables\n\n\nknitr\nGenerate reports in various formats. Part of the R markdown language\n\n\nlabelled\nManipulate variable and response labels.\n\n\nlubridate\nPart of the tidyverse package, useful for manipulating dates.\n\n\nreadr\nPart of the larger tidyverse package, useful for writing R’s own data format RDS.\n\n\nreadxl\nRead data from .xls or .xlsx format\n\n\nsampling\nDraw samples from a data frame.\n\n\nsdcMicro\nPerform statistical disclosure control on a dataset.\n\n\nstringr\nPart of the larger tidyverse package.\n\n\nsurvey\nPerform various statistics and models on a survey dataset, applying survey weights where applicable.\n\n\ntidyr\nPart of the larger tidyverse package.\n\n\ntidyverse\nA set of packages used to manipulate, clean and visualize data including (but not limited to) dplyr, ggplot2, lubridate, readr, stringr and tidyr.\n\n\nVIM\nTools to create graphical representations of data. Can be used to visualize missing and imputed data.\n\n\nwritexl\nExport data frame to .xlsx format"
  },
  {
    "objectID": "annex2.html#sec-9addlabels",
    "href": "annex2.html#sec-9addlabels",
    "title": "Appendix B — Annex - Metadata guidance",
    "section": "B.1 Add/modify labels",
    "text": "B.1 Add/modify labels\n\nB.1.1 Variable labels\nThe MDL needs both the variable names and labels for the metadata on the MDL website. This facilitates the detailed search of the contents in each data file. See an example below:\n\n\n\n\n\nA dataset may or may not have variable labels attached to each variable. Normally they are only attached for stata or spss files. Even if they did have labels, key variables often lose their labels after going through sdcMicro and/or some labels may need to be modified if the variables were modified (e.g. recoded). The following code can be used to add labels to a data frame in R from an Excel file with the list of variable names and variable labels.\nAdding labels in R\nGiven the dataset your_dataset and an Excel file data_labels.xlsx with two columns that include variable names (var_names) and variable labels (var_labels), you can use the labelled package and the following code to add labels.\n\n\nCode\n# Install package & add library\ninstall.packages('labelled')\nlibrary(labelled)\n\n# Import the mapping file. If you use a csv, you can use read.csv. If you use an excel file you can use readxl::read_excel()\nlabels_map <- readxl::read_excel('2_documentation/data_labels.xlsx')\n\n# Labeling function\nlabel_from_mapping <- function(a_dataset, var_names, var_labels){\n  \n  for(i in 1:length(names(a_dataset))){\n    a_name = names(a_dataset)[i]\n    if(!a_name %in% var_names){\n      print(paste('Label not found for var: ',a_name))\n    }else{\n      index = match(a_name, var_names)\n      a_label = var_labels[index]\n      #print(paste('setting to var: ', a_name, ' the label: ', a_label))\n      var_label(a_dataset[a_name]) = a_label\n    }\n  }\n  return(a_dataset)\n}\n\n# Apply labels\nyour_dataset <- label_from_mapping(your_dataset, labels_map$var_name, labels_map$var_label)\n#view the result\nView(your_dataset)\n\n\n \n\n\nB.1.2 Value labels\nValue labels are descriptions of the values a variable can take. Value labels are particularly important for categorical variables whose value names are a shortened, abbreviated or an encoded version of the response option (e.g. 0 = no, 1 = yes). If the value names are shortened, abbreviated, or encoded AND documentation in a convenient format is available (i.e in a codebook or the questionnaire), then the value names can be shared in their shortened, abbreviated or encoded format. If documentation is not available in a convenient format or cannot be shared, either:\n\nthe value names should be converted to their labels (see example below); or\na codebook should be created (see example under section Save anonymous version, codebook and anonymization script).\n\n \nReplacing value names in R\n\n\nCode\n# Replace codes with labels\nyour_dataset$your_variable[your_dataset$your_variable =='1'] <- 'Yes'\nyour_dataset$your_variable[your_dataset$your_variable =='0'] <- 'No'\n\n# or alternatively \nyour_dataset %>%\n     mutate(your_variable = recode(your_variable, '1' = 'Yes', '0' = 'No'))\n\n#The following is a more efficient way to replace the codes across multiple variables.\n# Replace codes with labels for multiple variables\nmap_values <- function(yesno_var){\n     yesno_var = case_when(\n     yesno_var == 1 ~ 'yes',\n     yesno_var == 0 ~ 'no'\n     )\n     return(yesno_var)\n}\n\nyour_dataset <- mutate(your_dataset, across( c(your_var1, your_var2, your_var3), map_values) )"
  },
  {
    "objectID": "annex2.html#sec-9fields",
    "href": "annex2.html#sec-9fields",
    "title": "Appendix B — Annex - Metadata guidance",
    "section": "B.2 Metadata fields",
    "text": "B.2 Metadata fields\nMetadata created under the DDI standard has 4 main sections:\n\nDataset (study) level: General information on the dataset as a whole\nData file (resource) level: Information on the data file itself, including its creation date and version number\nVariable Level: Information on the variables in a data file include the variable names, labels and response rate\nOther files (resource) level: General information on other resources such as questionnaires, reports, summary analyses, etc.\n\nThe following tables provide an overview of the mandatory metadata fields in RIDL and the MDL, and instructions for their use. Some fields are not mandatory in the tools (denoted with an *) however they are still included in the table because they are necessary for the curation process.\nNote: While it is not necessary to complete all the fields, the metadata should include all the required fields to facilitate responsible use of the data. When populating the metadata, assume that any users may not have access to the Data Provider to seek more information than is what is available in the metadata.\n\nB.2.1 Mandatory fields at the dataset level\n\n\n\n\n\n\n\n\nField RIDL\nField MDL\nDescription & usage\n\n\n\n\nTitle*\nTitle*\nThe dataset study title should follow, when possible, the naming convention used by UNHCR’s Operational Data Portal which is generally\nCountry: Subject - Mmm YYYY. Example: Zimbabwe: Socio-economic assessment in Camp 1 — 2017.\n\n\nnot included\nStudy type*\nControlled field for broad category defining the dataset. List of possible responses includes:\n1-2-3 Survey, phase 1 [hh/123-1]\nAdministrative Records, Health (ad/hea]\nAdministrative Records, Education (ad/edu]\nAdministrative Records, Other (ad/oth]\nAgricultural Census [ag/census]\nAgricultural Survey [ag/oth]\nChild Labor Survey [hh/cls]\nCore Welfare Indicators Questionnaire [hh/cwiq]\nDemographic and Health Survey [hh/dhs]\nEnterprise Survey [en/oth]\nEnterprise Census [en/census]\nIncome/Expenditure/Household Survey [hh/ies]\nInformal Sector Survey [hh/iss]\nIntegrated Survey (non-LSMS) [hh/is]\nLabor Force Survey [hh/lfs]\nLiving Standards Measurement Study [hh/lsms]\nOther Household Health Survey [hh/hea]\nOther Household Survey [hh/oth]\nPrice Survey [hh/prc]\nPriority Survey (hh/ps]\nPopulation and Housing Census [hh/popcen]\nSample Frame, Households [sf/hh]\nSample Frame, Enterprises [sf/en]\nService Provision Assessments [hh/spa]\nSocio-Economic/Monitoring Survey [hh/sems]\nStatistical Info. & Monitoring Prog. [hh/simpoc]\nWorld Fertility Survey [hh/wfs]\nWorld Health Survey [hh/whs]\n\n\nnot included\nID Number\nUnique study identifier. Should follow the same structures as outlined in section 2.1.2 Dataset identifier.\n\n\nURL\nnot included\nThe URL link to the data on RIDL. This is automatically filled in RIDL, however, should be modified so that it is the same as the dataset identifier (see section 2.1.2 Dataset identifier). For example:\nhttps://ridl.unhcr.org/dataset/UNHCR_ZWE_2017_SENS_CAMP1\n\n\navailable at resource level metadata\nVersion Description\nShould describe the version of the data in the study:\nv01: Raw data\nv1.1: Clean data\nv2.1: Anonymized data\n\n\nDescription\nAbstract*\nSummary of the dataset including the purpose and objectives of the data collection exercise including any relevant background information, what was collected, from whom was it collected, when was it collected, how was it collected and by whom was it collected as well as any other information pertinent to be known by other Data Users. Note that the subject of the abstract should be the data (not the report or any other product that was developed using the data). Below is an example of good abstract:\n\n\n\n\n“Against the recent COVID-19 pandemic and its secondary socio-economic impact, UNHCR in partnership with WFP undertook a Joint Needs Assessment (JNA) in Mantapala settlement to identify livelihood challenges and opportunities and develop socio-economic profiles of the most vulnerable. The JNA was to inform programmatic decisions and suggest the most appropriate and feasible targeting approach for future interventions by UNHCR and WFP in the settlement.\nAn extensive literature review and technical discussions took place to identify the knowledge gap during the assessment design phase. At the time of the assessment, 4,076 households were living in the settlement. Primary data collection took place between 19 and 28 September 2020 by UNHCR and WFP field teams. This included five focus group discussions, two key informant interviews and 1,128 household interviews. The household interviews used a structured questionnaire. Simple random sampling was used, and findings are statistically representative at the settlement level\nThis dataset is the anonymous version of the household interview data collected through the structured questionnaire. It was processed jointly by UNHCR and WFP\n\n\nOperational purpose of data\nKind of Data\nIn the MDL, this field is a broad classification of the data from the following controlled vocabulary:\nSample survey data\nCensus/enumeration data\nAdministrative records data\nAggregate data\nClinical data\nEven/trasaction data\nObservation data/ratings\nProcess-produced data\nTime-budget diaries\n\n\n\n\nIn RIDL, the field is called ‘Operational purpose of data’ and allows multiple selection from the following list:\nParticipatory assessments\nBaseline Household Survey\nRapid Needs Assessment\nProtection monitoring\nProgramme Monitoring\nPopulation data\nCartography, infrastructure and GIS\nResults monitoring\n\n\nnot included\nDescription of scope\nDescription of the subject matter covered in the dataset. Note: this is not geographic scope but thematic.\n\n\n\nCountry\nName of country(ies) covered by the dataset, as well as its ISO-3 Code. See guidance on country names in the UNHCR Style Companion, March 2019.\n\n\nGeographies\nGeographic Coverage\nDescribes the geographic units this data has been collected in. In the case of RIDL, the most detailed geographical unit needs to be selected.\n\n\n-\nUniverse\nPopulation of interest for the whole study\n\n\nData Container\n-\nData container on RIDL.\n\n\nExternal Access Level\n-\nOnly relevant for microdata to be published on the MDL. Single select on access level on MDL.\n\n\n-\nPrimary investigator\nName of the institution/organization(s) implementing the study. This may be different from the data collector if the institution/organization(s) recruited a third party to collect the data.\n\n\nData Collector\nData collector(s)\nName of the institution/organization(s) that collected the data for the study.\n\n\nTopic classification\nTopics\nTopic(s) covered by the study. This is a multiple select, with predefined options available in RIDL.\n\n\nUnit of measurement\nUnits of analysis*\nUnit of observation / subject of the data. For example, if the data subjects were households, the unit of observation is the household, if it was individuals, individuals, etc.\n\n\nSampling Procedure*\nSampling procedure*\nMultiple select on sampling method if relevant. On MDL, open text field.\n\n\nData collectiontechnique\nData collection mode\nSingle select on method used to collect the data.\n\n\nArchived\n-\nYes/No if data collection and manipulation is complete and data are to be archived. This should remain ‘yes’ if data collection/manipulation is still active.\n\n\nAdmin notes – Sampling procedure*\n-\nIf the data are a sample, this section must include information on the sample frame (including the size of the sample frame), sample size and further details on the sampling methodology.\n\n\nAdmin notes – Access authority*\nAccess authority\nThe name and organization of the data controller(s).\n\n\n\n\nFor RIDL, this should be the name of the UNHCR staff member that is considered the data controller. If the data is co-controlled by another organization, this should also include the name of other organizations that have control over the data.\n\n\n\n\nFor MDL, this should be UNHCR with the contact information as microdata@unhcr.org.\n\n\n-\nCitation Requirement\nThe citation requirements for MDL Data Users in the following format: Data Controller (YYYY). Study title, YYYY. Accessed from https://microdata.unhcr.org.\n\n\n\n\nExample: UNHCR (2021). Burkina Faso: Standardized Expanded Nutrition Survey microdata, 2014. Accessed from: https://microdata.unhcr.org.\nFor externally harvested datasets, the citation should remain as provided if there is one or follow the above format if there is not one available in the provided metadata.\n\n\n\n \n\n\nB.2.2 Mandatory fields at the resource level\n\n\n\n\n\n\n\n\nField RIDL\nField MDL\nDescription & usage\n\n\n\n\nName*\nName*\nBrief name of the file. This should include if the data are raw, clean or anonymous. Use the following naming conventions to distinguish between the three:\ndata (raw v0.x)\ndata (clean v1.x)\ndata (anonymous v2.x)\n\n\nFile type\n\nType of file such as .xlsx, .csv, .dta, etc. This is automatically populated based on the file uploaded.\n\n\nInternal Access Level\n\nSingle select on who within UNHCR should have access to the data. Either internally visible (anyone in UNHCR can have access) or private (only certain individuals should have access).\n\n\nData collection first date\nData collection dates – Start*\nFirst date of data collection\n\n\nData collection last date\nData collection dates – End*\nLast date of data collection\n\n\nVersion\nVersion\nVersion of the file\n\n\nFile Process Status\n\nThe process status of the file: Raw, Cleaned or Cleaned & Anonymized.\n\n\nIdentifiability\n\nThe level to which data subjects are identifiable in the data. Select between Personally identifiable (i.e. direct identifiers included), Anonymized 1st level (direct identifiers removed), Anonymized 2nd level (anonymized using UNHCR standards and for release under Licensed Use), Anonymized 3rd level (anonymized using UNHCR standards and for release under Public Use)."
  },
  {
    "objectID": "annex3.html",
    "href": "annex3.html",
    "title": "Appendix C — Annex: Key and sensitive variables",
    "section": "",
    "text": "The following table is a non-exhaustive list of some of the common key and sensitive variables found in operational microdata collected by UNHCR and partners, and some example actions that could be performed as needed.\nPlease note that the following variables may be considered key variables in one dataset and not another depending on the context, and the actions may or may not be relevant depending on the dataset and the context.\n\n\n\nVariable\nExample actions\n\n\n\n\nUnique ID\nReplace with pseudonym (see section Unique ID)\n\n\nSample weight\nIn the case of stratified sampling, the weight will have the same value for all records in the same strata. If the variable used to define the strata is a key variable, local suppression of that key variable should be avoided.\n\n\n\nIf local suppressions are done on the strata variable, then the record will have to be removed as the response to the strata variable is obvious from the weight variable.\n\n\nHousehold size\nConvert to factor and treated as a categorical key variable.\n\n\n\nTop code larger households, e.g.: 8+\n\n\n\nRecode into groups useful to analysis, e.g.: 1, 2, 3-4, 5-6, 7-8, >8\n\n\n\nIn the case of a hierarchical dataset, household size may be able to be recalculated by counting the number of individuals for each household in the individual dataset linked to the household dataset. In such a case, household size cannot be recoded without modifying the individual responses.\n\n\nBreakdown of household members by age, sex, etc.\nOnly preserve level of detail useful to analysis.\n\n\n\nFor household-level files, replace with ratio, e.g. proportion women, proportion men, proportion under 18, etc.\n\n\nLocation variables\nAdministrative divisions – In cases where some divisions have few observations, they can be regrouped into an ‘other’ category, for example. In cases where there are multiple administrative divisions in the dataset (e.g., admin 1, admin 2 and admin 3), the lower level (e.g., admin 3) could be removed if not used in the primary analysis and/or for weights.\n\n\n\nPopulated place – In cases where some places have few observations, they can be recoded into an ‘other’ category. In cases where the populated place is too detailed and not used in primary analysis and/or weights and there is another location variable (such as administrative division), it may be decided to remove this variable all together.\n\n\nMarital status\nRecode to ‘married’, ‘widower’, ‘separated/divorced’, and ‘single’ households. If polygamous households are common, the level ‘married’ should be divided accordingly.\n\n\nAge\nConvert to factor and treated as a categorical key variable.\n\n\n\nRecode into groups useful to analysis, e.g., <18, 18-59 and >59 or five- or ten-year intervals.\n\n\n\nIn the case of SENS surveys children’s module, age in months should be rounded to the nearest month but not grouped as that would be removed the value of the variable (see section 11.6.2.2).\n\n\nGender\nIn most cases this variable contains only two categories (male/female). In case other categories are present (e.g.: transgender etc.) they may be recoded under the category ‘other’ if appropriate.\n\n\nCountry of origin, country of birth, nationality, etc.\nIn cases where some categories have few observations, they can be regrouped into a different category or an ‘other’ category.\n\n\nVariables related to sexual and gender-based violence.\nThis should be discussed with the Data Provider.\n\n\nLanguage\nIn cases where some categories have few observations, they can be regrouped into a different category or an ‘other’ category.\n\n\nIncome\nRound to avoid exact matching.\n\n\n\nTop code outliers.\n\n\n\nRecode into groups useful for analysis.\n\n\nOpen text variables\nSee section Free text variables."
  }
]